{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial\n",
    "\n",
    "In this seminar, we're going to play with [Tensorflow](https://www.tensorflow.org/) and see how it helps us build deep learning models.\n",
    "\n",
    "If you're running this notebook outside course environment, you'll need to install tensorflow:\n",
    "* `pip install tensorflow` should install cpu-only TF on Linux & Mac OS\n",
    "* If you want GPU support from offset, see [TF install page](https://www.tensorflow.org/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "For starters, let's implement a python function that computes the sum of squares of numbers from 0 to N-1.\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    return np.sum(np.arange(N) ** 2)#<student.Implement_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 104 ms, total: 269 ms\n",
      "Wall time: 268 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662921401752298880"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_squares(10 ** 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensoflow teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "#I gonna be your function parameter\n",
    "N = tf.placeholder('int64', name=\"input_to_your_function\")\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = tf.reduce_sum((tf.range(N) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662921401752298880\n",
      "CPU times: user 458 ms, sys: 274 ms, total: 731 ms\n",
      "Wall time: 587 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#example of computing the same as sum_squares\n",
    "print(result.eval({N: 10 ** 8}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 414 ms, sys: 2.71 ms, total: 416 ms\n",
      "Wall time: 265 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[662921401752298880, 662921401752298881]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s.run([result, result + 1], {N: 10 ** 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 664 ms, sys: 28.2 ms, total: 692 ms\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result.eval({N: 10 ** 8})\n",
    "(result + 1).eval({N: 10 ** 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the last two times. Why do you think they're so different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "1. define placeholders where you'll send inputs;\n",
    "2. make symbolic graph: a recipe for mathematical transformation of those placeholders;\n",
    "3. compute outputs of your graph with particular values for each placeholder\n",
    "  * output.eval({placeholder:value}) \n",
    "  * s.run(output, {placeholder:value})\n",
    "\n",
    "* So far there are two main entities: \"placeholder\" and \"transformation\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be int32/64, floats of booleans (uint8) of various size.\n",
    "\n",
    "* You can define new transformations as an arbitrary operation on placeholders and other transformations\n",
    " * tf.reduce_sum(tf.arange(N)\\**2) are 3 sequential transformations of placeholder N\n",
    " * There's a tensorflow symbolic version for every numpy function\n",
    "   * `a+b, a/b, a**b, ...` behave just like in numpy\n",
    "   * np.mean -> tf.reduce_mean\n",
    "   * np.arange -> tf.range\n",
    "   * np.cumsum -> tf.cumsum\n",
    "   * If if you can't find the op you need, see the docs [docs](https://www.tensorflow.org/api_docs/python).\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Default placeholder that can be arbitrary float32 scalar, vertor, matrix, etc.\n",
    "arbitrary_input = tf.placeholder('float32')\n",
    "\n",
    "#Input vector of arbitrary length\n",
    "input_vector = tf.placeholder('float32', shape=(None,))\n",
    "\n",
    "#Input vector that _must_ have 10 elements and integer type\n",
    "fixed_vector = tf.placeholder('int32', shape=(10,))\n",
    "\n",
    "#Matrix of arbitrary n_rows and 15 columns (e.g. a minibatch your data table)\n",
    "input_matrix = tf.placeholder('float32', shape=(None, 15))\n",
    "\n",
    "#You can generally use None whenever you don't need a specific shape\n",
    "input1 = tf.placeholder('float64', shape=(None, 100, None))\n",
    "input2 = tf.placeholder('int32', shape=(None, None, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#elementwise multiplication\n",
    "double_the_vector = input_vector * 2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = tf.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector ** 2 - input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Practice time:\n",
    "#create two vectors of type float32\n",
    "my_vector = tf.placeholder('float32', shape=(None,))\n",
    "my_vector2 = tf.placeholder('float32', shape=(None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = my_vector * my_vector2 /  (tf.sin(my_vector) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(my_transformation)\n",
    "#it's okay, it's a symbolic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.62913239,  2.09501147,  2.62899613,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.arange(5).astype('float32')\n",
    "my_transformation.eval({my_vector: dummy, my_vector2: dummy[::-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing graphs\n",
    "\n",
    "It's often useful to visualize the computation graph when debugging or optimizing. \n",
    "Interactive visualization is where tensorflow really shines as compared to other frameworks. \n",
    "\n",
    "There's a special instrument for that, called Tensorboard. You can launch it from console:\n",
    "\n",
    "```tensorboard --logdir=/tmp/tboard --port=7007```\n",
    "\n",
    "If you're pathologically afraid of consoles, try this:\n",
    "\n",
    "```os.system(\"tensorboard --logdir=/tmp/tboard --port=7007 &\"```\n",
    "\n",
    "_(but don't tell anyone we taught you that)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#launch tensorflow the ugly way, uncomment if you need that\n",
    "#import os\n",
    "#!killall tensorboard\n",
    "#os.system(\"tensorboard --logdir=/tmp/tboard --port=7007 &\")\n",
    "\n",
    "#show graph to tensorboard\n",
    "writer = tf.summary.FileWriter(\"/tmp/tboard\", graph=tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic functionality of tensorboard is drawing graphs. One you've run the cell above, go to `localhost:7007` in your browser and switch to _graphs_ tab in the topbar. \n",
    "\n",
    "Here's what you should see:\n",
    "\n",
    "<img src=\"https://s12.postimg.org/a374bmffx/tensorboard.png\" width=480>\n",
    "\n",
    "Tensorboard also allows you to draw graphs (e.g. learning curves), record images & audio ~~and play flash games~~. This is useful when monitoring learning progress and catching some training issues.\n",
    "\n",
    "One researcher said:\n",
    "```\n",
    "If you spent last four hours of your worktime watching as your algorithm prints numbers and draws figures, you're probably doing deep learning wrong.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more on tensorboard usage [here](https://www.tensorflow.org/get_started/graph_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself\n",
    "\n",
    "__[2 points max]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "vector_1 = tf.placeholder('float32', shape=(None,))\n",
    "vector_2 = tf.placeholder('float32', shape=(None,))\n",
    "\n",
    "mse = tf.reduce_mean((vector_1 - vector_2) ** 2)\n",
    "\n",
    "compute_mse = lambda vector1, vector2: mse.eval({vector_1: vector1, vector_2: vector2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1, 5, 10, 10**3]:\n",
    "    \n",
    "    elems = [np.arange(n), np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n), np.random.random(n), np.random.randint(100, size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el, el_2))\n",
    "            my_mse = compute_mse(el, el_2)\n",
    "            if not np.allclose(true_mse, my_mse):\n",
    "                print('Wrong result:')\n",
    "                print('mse(%s,%s)'%(el, el_2))\n",
    "                print(\"should be: %f, but your function returned %f\"%(true_mse, my_mse))\n",
    "                raise(ValueError, \"Что-то не так\")\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables\n",
    "\n",
    "The inputs and transformations have no value outside function call. This isn't too comfortable if you want your model to have parameters (e.g. network weights) that are always present, but can change their value over time.\n",
    "\n",
    "Tensorflow solves this with `tf.Variable` objects.\n",
    "* You can assign variable a value at any time in your graph\n",
    "* Unlike placeholders, there's no need to explicitly pass values to variables when `s.run(...)`-ing\n",
    "* You can use variables the same way you use transformations \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = tf.Variable(initial_value=np.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value [ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#initialize variable(s) with initial values\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print (\"initial value\", s.run(shared_vector_1))\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new value [ 0.  1.  2.  3.  4.]\n"
     ]
    }
   ],
   "source": [
    "#setting new value\n",
    "s.run(shared_vector_1.assign(np.arange(5)))\n",
    "\n",
    "#getting that new value\n",
    "print (\"new value\", s.run(shared_vector_1))\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.gradients - why graphs matter\n",
    "* Tensorflow can compute derivatives and gradients automatically using the computation graph\n",
    "* Gradients are computed as a product of elementary derivatives via chain rule:\n",
    "\n",
    "$$ {\\partial f(g(x)) \\over \\partial x} = {\\partial f(g(x)) \\over \\partial g(x)}\\cdot {\\partial g(x) \\over \\partial x} $$\n",
    "\n",
    "It can get you the derivative of any graph as long as it knows how to differentiate elementary operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_scalar = tf.placeholder('float32')\n",
    "\n",
    "scalar_squared = my_scalar ** 2\n",
    "\n",
    "#a derivative of scalar_squared by my_scalar\n",
    "derivative = tf.gradients(scalar_squared, my_scalar)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5ce6257fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBO5v8ABESeG0Mi6AUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce655efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "x_squared, x_squared_der = s.run(\n",
    "    [scalar_squared, derivative],\n",
    "    {my_scalar: x}\n",
    ")\n",
    "\n",
    "plt.plot(x, x_squared, label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vector = tf.placeholder('float32', shape=(None,))\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = tf.reduce_mean((my_vector + my_scalar) ** (1 + tf.nn.moments(my_vector, [0])[1]) + 1. / tf.atan(my_scalar)) / (my_scalar ** 2 + 1) + 0.01 * tf.sin(2 * my_scalar ** 1.5) * (tf.reduce_sum(my_vector) * my_scalar ** 2) * tf.exp((my_scalar-4)**2)/(1+tf.exp((my_scalar-4)**2))*(1.-(tf.exp(-(my_scalar-4)**2))/(1+tf.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar, der_by_vector = tf.gradients(weird_psychotic_function, [my_scalar, my_vector])#<student.compute_grad_over_scalar_and_vector()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5ce5b55630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL4QQPYQEfCGE6CH+P50m1qrKeMiaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d6c0582e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting your derivative\n",
    "scalar_space = np.linspace(1, 7, 100)\n",
    "\n",
    "y = [\n",
    "    s.run(weird_psychotic_function, {my_scalar: x, my_vector: [1, 2, 3]})\n",
    "    for x in scalar_space\n",
    "]\n",
    "\n",
    "plt.plot(scalar_space, y, label='function')\n",
    "\n",
    "y_der_by_scalar = [\n",
    "    s.run(der_by_scalar, {my_scalar: x, my_vector: [1, 2, 3]})\n",
    "    for x in scalar_space\n",
    "]\n",
    "\n",
    "plt.plot(scalar_space, y_der_by_scalar, label='derivative')\n",
    "plt.grid(); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - optimizers\n",
    "\n",
    "While you can perform gradient descent by hand with automatic grads from above, tensorflow also has some optimization methods implemented for you. Recall momentum & rmsprop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_guess = tf.Variable(np.zeros(2, dtype='float32'))\n",
    "y_true = tf.range(1, 3, dtype='float32')\n",
    "\n",
    "loss = tf.reduce_mean((y_guess - y_true + tf.random_normal([2])) ** 2) \n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss, var_list=[y_guess])\n",
    "\n",
    "#same, but more detailed:\n",
    "#opt = tf.train.MomentumOptimizer(0.01,0.9)\n",
    "#updates = opt.compute_gradients(loss, [y_guess])\n",
    "#optimizer = opt.apply_gradients(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoMU7iIiMUjhLiISgxTuIiIx6P8DxGiIwg/2Ex4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce52ad160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "guesses = [s.run(y_guess)]\n",
    "\n",
    "for _ in range(100):\n",
    "    s.run(optimizer)\n",
    "    guesses.append(s.run(y_guess))\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.plot(*zip(*guesses), marker='.')\n",
    "    plt.scatter(*s.run(y_true), c='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "__[ 4 points max]__\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights and biases fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Define loss function and optimizer\n",
    "* Train regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (1797,)]:\n",
      "X [shape - (1797, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits()\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "num_features = X.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "print(\"y [shape - %s]:\"%(str(y.shape)))\n",
    "\n",
    "print(\"X [shape - %s]:\"%(str(X.shape)))\n",
    "print(X[:1])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 10)\n"
     ]
    }
   ],
   "source": [
    "# make target in one-hot format\n",
    "\n",
    "y = tf.one_hot(y, 10).eval()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5ce58586d8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuo92pLZ1pQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce5cc6828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% we can visualize any one of the images by reshaping it to a 8x8 image\n",
    "plt.imshow(np.reshape(mnist.data[0, :], (8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "weights = tf.get_variable(\"weights\", shape=(num_features, num_classes), initializer=tf.glorot_uniform_initializer())\n",
    "bias = tf.get_variable(\"bias\", shape=(num_classes,), initializer=tf.glorot_uniform_initializer())\n",
    "input_X = tf.placeholder('float32', shape=(None, num_features))\n",
    "input_y = tf.placeholder('float32', shape=(None, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = tf.nn.softmax(tf.matmul(input_X, weights) + bias)\n",
    "loss = -tf.reduce_mean(input_y * tf.log(predicted_y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss, var_list=[weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy is done for you\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predicted_y, 1),\n",
    "                                           tf.argmax(input_y, 1)), \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:2.0005\n",
      "train auc: 0.0801782\n",
      "test auc: 0.0666667\n",
      "loss at iter 1:1.9551\n",
      "train auc: 0.0794358\n",
      "test auc: 0.0688889\n",
      "loss at iter 2:1.9107\n",
      "train auc: 0.0794358\n",
      "test auc: 0.0666667\n",
      "loss at iter 3:1.8673\n",
      "train auc: 0.0809206\n",
      "test auc: 0.0688889\n",
      "loss at iter 4:1.8251\n",
      "train auc: 0.0831477\n",
      "test auc: 0.0666667\n",
      "loss at iter 5:1.7842\n",
      "train auc: 0.081663\n",
      "test auc: 0.0733333\n",
      "loss at iter 6:1.7446\n",
      "train auc: 0.0838901\n",
      "test auc: 0.0733333\n",
      "loss at iter 7:1.7063\n",
      "train auc: 0.0831477\n",
      "test auc: 0.0755556\n",
      "loss at iter 8:1.6693\n",
      "train auc: 0.0868597\n",
      "test auc: 0.0755556\n",
      "loss at iter 9:1.6337\n",
      "train auc: 0.0935412\n",
      "test auc: 0.0777778\n",
      "loss at iter 10:1.5994\n",
      "train auc: 0.0987379\n",
      "test auc: 0.08\n",
      "loss at iter 11:1.5662\n",
      "train auc: 0.0972532\n",
      "test auc: 0.0822222\n",
      "loss at iter 12:1.5341\n",
      "train auc: 0.100223\n",
      "test auc: 0.0844444\n",
      "loss at iter 13:1.5031\n",
      "train auc: 0.100965\n",
      "test auc: 0.0888889\n",
      "loss at iter 14:1.4728\n",
      "train auc: 0.10245\n",
      "test auc: 0.0955556\n",
      "loss at iter 15:1.4433\n",
      "train auc: 0.104677\n",
      "test auc: 0.106667\n",
      "loss at iter 16:1.4145\n",
      "train auc: 0.105419\n",
      "test auc: 0.102222\n",
      "loss at iter 17:1.3861\n",
      "train auc: 0.103192\n",
      "test auc: 0.0977778\n",
      "loss at iter 18:1.3582\n",
      "train auc: 0.10245\n",
      "test auc: 0.102222\n",
      "loss at iter 19:1.3306\n",
      "train auc: 0.101707\n",
      "test auc: 0.102222\n",
      "loss at iter 20:1.3034\n",
      "train auc: 0.104677\n",
      "test auc: 0.102222\n",
      "loss at iter 21:1.2765\n",
      "train auc: 0.10245\n",
      "test auc: 0.1\n",
      "loss at iter 22:1.2500\n",
      "train auc: 0.101707\n",
      "test auc: 0.0977778\n",
      "loss at iter 23:1.2237\n",
      "train auc: 0.101707\n",
      "test auc: 0.0955556\n",
      "loss at iter 24:1.1978\n",
      "train auc: 0.10245\n",
      "test auc: 0.0933333\n",
      "loss at iter 25:1.1723\n",
      "train auc: 0.10245\n",
      "test auc: 0.0955556\n",
      "loss at iter 26:1.1471\n",
      "train auc: 0.101707\n",
      "test auc: 0.0977778\n",
      "loss at iter 27:1.1223\n",
      "train auc: 0.100223\n",
      "test auc: 0.0977778\n",
      "loss at iter 28:1.0979\n",
      "train auc: 0.0994803\n",
      "test auc: 0.0977778\n",
      "loss at iter 29:1.0740\n",
      "train auc: 0.100223\n",
      "test auc: 0.0977778\n",
      "loss at iter 30:1.0505\n",
      "train auc: 0.0972532\n",
      "test auc: 0.0977778\n",
      "loss at iter 31:1.0276\n",
      "train auc: 0.0965108\n",
      "test auc: 0.0955556\n",
      "loss at iter 32:1.0051\n",
      "train auc: 0.0987379\n",
      "test auc: 0.0955556\n",
      "loss at iter 33:0.9832\n",
      "train auc: 0.0979955\n",
      "test auc: 0.0911111\n",
      "loss at iter 34:0.9619\n",
      "train auc: 0.095026\n",
      "test auc: 0.0888889\n",
      "loss at iter 35:0.9411\n",
      "train auc: 0.0920564\n",
      "test auc: 0.0911111\n",
      "loss at iter 36:0.9209\n",
      "train auc: 0.0920564\n",
      "test auc: 0.0888889\n",
      "loss at iter 37:0.9013\n",
      "train auc: 0.0920564\n",
      "test auc: 0.0888889\n",
      "loss at iter 38:0.8823\n",
      "train auc: 0.0927988\n",
      "test auc: 0.0911111\n",
      "loss at iter 39:0.8639\n",
      "train auc: 0.0935412\n",
      "test auc: 0.0911111\n",
      "loss at iter 40:0.8461\n",
      "train auc: 0.0942836\n",
      "test auc: 0.0888889\n",
      "loss at iter 41:0.8290\n",
      "train auc: 0.0965108\n",
      "test auc: 0.0888889\n",
      "loss at iter 42:0.8124\n",
      "train auc: 0.0972532\n",
      "test auc: 0.0933333\n",
      "loss at iter 43:0.7964\n",
      "train auc: 0.0987379\n",
      "test auc: 0.0933333\n",
      "loss at iter 44:0.7809\n",
      "train auc: 0.101707\n",
      "test auc: 0.0977778\n",
      "loss at iter 45:0.7661\n",
      "train auc: 0.104677\n",
      "test auc: 0.102222\n",
      "loss at iter 46:0.7517\n",
      "train auc: 0.107647\n",
      "test auc: 0.102222\n",
      "loss at iter 47:0.7379\n",
      "train auc: 0.112843\n",
      "test auc: 0.102222\n",
      "loss at iter 48:0.7245\n",
      "train auc: 0.115813\n",
      "test auc: 0.104444\n",
      "loss at iter 49:0.7116\n",
      "train auc: 0.11804\n",
      "test auc: 0.106667\n",
      "loss at iter 50:0.6990\n",
      "train auc: 0.123979\n",
      "test auc: 0.111111\n",
      "loss at iter 51:0.6869\n",
      "train auc: 0.127691\n",
      "test auc: 0.111111\n",
      "loss at iter 52:0.6751\n",
      "train auc: 0.132146\n",
      "test auc: 0.117778\n",
      "loss at iter 53:0.6636\n",
      "train auc: 0.132888\n",
      "test auc: 0.12\n",
      "loss at iter 54:0.6524\n",
      "train auc: 0.137342\n",
      "test auc: 0.12\n",
      "loss at iter 55:0.6414\n",
      "train auc: 0.143281\n",
      "test auc: 0.124444\n",
      "loss at iter 56:0.6306\n",
      "train auc: 0.14922\n",
      "test auc: 0.137778\n",
      "loss at iter 57:0.6201\n",
      "train auc: 0.157387\n",
      "test auc: 0.142222\n",
      "loss at iter 58:0.6098\n",
      "train auc: 0.161099\n",
      "test auc: 0.144444\n",
      "loss at iter 59:0.5996\n",
      "train auc: 0.165553\n",
      "test auc: 0.148889\n",
      "loss at iter 60:0.5897\n",
      "train auc: 0.169265\n",
      "test auc: 0.153333\n",
      "loss at iter 61:0.5800\n",
      "train auc: 0.174462\n",
      "test auc: 0.162222\n",
      "loss at iter 62:0.5704\n",
      "train auc: 0.178174\n",
      "test auc: 0.164444\n",
      "loss at iter 63:0.5611\n",
      "train auc: 0.180401\n",
      "test auc: 0.173333\n",
      "loss at iter 64:0.5519\n",
      "train auc: 0.18634\n",
      "test auc: 0.18\n",
      "loss at iter 65:0.5430\n",
      "train auc: 0.194506\n",
      "test auc: 0.18\n",
      "loss at iter 66:0.5342\n",
      "train auc: 0.198218\n",
      "test auc: 0.188889\n",
      "loss at iter 67:0.5257\n",
      "train auc: 0.205642\n",
      "test auc: 0.193333\n",
      "loss at iter 68:0.5174\n",
      "train auc: 0.210097\n",
      "test auc: 0.202222\n",
      "loss at iter 69:0.5093\n",
      "train auc: 0.216036\n",
      "test auc: 0.213333\n",
      "loss at iter 70:0.5014\n",
      "train auc: 0.221232\n",
      "test auc: 0.213333\n",
      "loss at iter 71:0.4936\n",
      "train auc: 0.225687\n",
      "test auc: 0.215556\n",
      "loss at iter 72:0.4861\n",
      "train auc: 0.234595\n",
      "test auc: 0.22\n",
      "loss at iter 73:0.4787\n",
      "train auc: 0.240535\n",
      "test auc: 0.228889\n",
      "loss at iter 74:0.4715\n",
      "train auc: 0.246474\n",
      "test auc: 0.24\n",
      "loss at iter 75:0.4645\n",
      "train auc: 0.250928\n",
      "test auc: 0.242222\n",
      "loss at iter 76:0.4576\n",
      "train auc: 0.25761\n",
      "test auc: 0.244444\n",
      "loss at iter 77:0.4508\n",
      "train auc: 0.265776\n",
      "test auc: 0.246667\n",
      "loss at iter 78:0.4441\n",
      "train auc: 0.271715\n",
      "test auc: 0.255556\n",
      "loss at iter 79:0.4376\n",
      "train auc: 0.279881\n",
      "test auc: 0.264444\n",
      "loss at iter 80:0.4311\n",
      "train auc: 0.284336\n",
      "test auc: 0.266667\n",
      "loss at iter 81:0.4248\n",
      "train auc: 0.295471\n",
      "test auc: 0.275556\n",
      "loss at iter 82:0.4186\n",
      "train auc: 0.299926\n",
      "test auc: 0.288889\n",
      "loss at iter 83:0.4125\n",
      "train auc: 0.310319\n",
      "test auc: 0.297778\n",
      "loss at iter 84:0.4065\n",
      "train auc: 0.314774\n",
      "test auc: 0.3\n",
      "loss at iter 85:0.4006\n",
      "train auc: 0.323682\n",
      "test auc: 0.302222\n",
      "loss at iter 86:0.3948\n",
      "train auc: 0.336303\n",
      "test auc: 0.304444\n",
      "loss at iter 87:0.3892\n",
      "train auc: 0.340015\n",
      "test auc: 0.313333\n",
      "loss at iter 88:0.3836\n",
      "train auc: 0.345954\n",
      "test auc: 0.322222\n",
      "loss at iter 89:0.3782\n",
      "train auc: 0.351151\n",
      "test auc: 0.326667\n",
      "loss at iter 90:0.3728\n",
      "train auc: 0.35412\n",
      "test auc: 0.333333\n",
      "loss at iter 91:0.3676\n",
      "train auc: 0.361544\n",
      "test auc: 0.34\n",
      "loss at iter 92:0.3625\n",
      "train auc: 0.368226\n",
      "test auc: 0.348889\n",
      "loss at iter 93:0.3574\n",
      "train auc: 0.37565\n",
      "test auc: 0.351111\n",
      "loss at iter 94:0.3525\n",
      "train auc: 0.380846\n",
      "test auc: 0.355556\n",
      "loss at iter 95:0.3477\n",
      "train auc: 0.389013\n",
      "test auc: 0.366667\n",
      "loss at iter 96:0.3430\n",
      "train auc: 0.394952\n",
      "test auc: 0.371111\n",
      "loss at iter 97:0.3383\n",
      "train auc: 0.399406\n",
      "test auc: 0.38\n",
      "loss at iter 98:0.3338\n",
      "train auc: 0.403118\n",
      "test auc: 0.384444\n",
      "loss at iter 99:0.3293\n",
      "train auc: 0.407572\n",
      "test auc: 0.391111\n",
      "loss at iter 100:0.3249\n",
      "train auc: 0.413512\n",
      "test auc: 0.397778\n",
      "loss at iter 101:0.3206\n",
      "train auc: 0.415739\n",
      "test auc: 0.408889\n",
      "loss at iter 102:0.3164\n",
      "train auc: 0.420193\n",
      "test auc: 0.415556\n",
      "loss at iter 103:0.3123\n",
      "train auc: 0.423163\n",
      "test auc: 0.417778\n",
      "loss at iter 104:0.3082\n",
      "train auc: 0.423905\n",
      "test auc: 0.422222\n",
      "loss at iter 105:0.3042\n",
      "train auc: 0.431329\n",
      "test auc: 0.431111\n",
      "loss at iter 106:0.3002\n",
      "train auc: 0.436526\n",
      "test auc: 0.435556\n",
      "loss at iter 107:0.2964\n",
      "train auc: 0.440238\n",
      "test auc: 0.442222\n",
      "loss at iter 108:0.2926\n",
      "train auc: 0.44395\n",
      "test auc: 0.444444\n",
      "loss at iter 109:0.2888\n",
      "train auc: 0.449889\n",
      "test auc: 0.448889\n",
      "loss at iter 110:0.2852\n",
      "train auc: 0.455828\n",
      "test auc: 0.448889\n",
      "loss at iter 111:0.2815\n",
      "train auc: 0.458797\n",
      "test auc: 0.451111\n",
      "loss at iter 112:0.2780\n",
      "train auc: 0.463252\n",
      "test auc: 0.455556\n",
      "loss at iter 113:0.2745\n",
      "train auc: 0.466221\n",
      "test auc: 0.46\n",
      "loss at iter 114:0.2711\n",
      "train auc: 0.472903\n",
      "test auc: 0.46\n",
      "loss at iter 115:0.2677\n",
      "train auc: 0.476615\n",
      "test auc: 0.464444\n",
      "loss at iter 116:0.2644\n",
      "train auc: 0.481069\n",
      "test auc: 0.464444\n",
      "loss at iter 117:0.2611\n",
      "train auc: 0.487751\n",
      "test auc: 0.466667\n",
      "loss at iter 118:0.2579\n",
      "train auc: 0.489978\n",
      "test auc: 0.475556\n",
      "loss at iter 119:0.2548\n",
      "train auc: 0.492205\n",
      "test auc: 0.482222\n",
      "loss at iter 120:0.2517\n",
      "train auc: 0.495174\n",
      "test auc: 0.484444\n",
      "loss at iter 121:0.2486\n",
      "train auc: 0.499629\n",
      "test auc: 0.491111\n",
      "loss at iter 122:0.2456\n",
      "train auc: 0.505568\n",
      "test auc: 0.493333\n",
      "loss at iter 123:0.2427\n",
      "train auc: 0.507795\n",
      "test auc: 0.5\n",
      "loss at iter 124:0.2398\n",
      "train auc: 0.508537\n",
      "test auc: 0.504444\n",
      "loss at iter 125:0.2369\n",
      "train auc: 0.512249\n",
      "test auc: 0.508889\n",
      "loss at iter 126:0.2341\n",
      "train auc: 0.516704\n",
      "test auc: 0.511111\n",
      "loss at iter 127:0.2314\n",
      "train auc: 0.520416\n",
      "test auc: 0.513333\n",
      "loss at iter 128:0.2286\n",
      "train auc: 0.524128\n",
      "test auc: 0.513333\n",
      "loss at iter 129:0.2260\n",
      "train auc: 0.527097\n",
      "test auc: 0.513333\n",
      "loss at iter 130:0.2234\n",
      "train auc: 0.532294\n",
      "test auc: 0.522222\n",
      "loss at iter 131:0.2208\n",
      "train auc: 0.536748\n",
      "test auc: 0.522222\n",
      "loss at iter 132:0.2182\n",
      "train auc: 0.539718\n",
      "test auc: 0.522222\n",
      "loss at iter 133:0.2157\n",
      "train auc: 0.541945\n",
      "test auc: 0.524444\n",
      "loss at iter 134:0.2133\n",
      "train auc: 0.54343\n",
      "test auc: 0.524444\n",
      "loss at iter 135:0.2109\n",
      "train auc: 0.548627\n",
      "test auc: 0.526667\n",
      "loss at iter 136:0.2085\n",
      "train auc: 0.551596\n",
      "test auc: 0.531111\n",
      "loss at iter 137:0.2061\n",
      "train auc: 0.554566\n",
      "test auc: 0.531111\n",
      "loss at iter 138:0.2038\n",
      "train auc: 0.55605\n",
      "test auc: 0.531111\n",
      "loss at iter 139:0.2016\n",
      "train auc: 0.559762\n",
      "test auc: 0.531111\n",
      "loss at iter 140:0.1993\n",
      "train auc: 0.562732\n",
      "test auc: 0.533333\n",
      "loss at iter 141:0.1971\n",
      "train auc: 0.564217\n",
      "test auc: 0.535556\n",
      "loss at iter 142:0.1950\n",
      "train auc: 0.568671\n",
      "test auc: 0.542222\n",
      "loss at iter 143:0.1928\n",
      "train auc: 0.573125\n",
      "test auc: 0.544444\n",
      "loss at iter 144:0.1907\n",
      "train auc: 0.576095\n",
      "test auc: 0.544444\n",
      "loss at iter 145:0.1887\n",
      "train auc: 0.580549\n",
      "test auc: 0.548889\n",
      "loss at iter 146:0.1866\n",
      "train auc: 0.582777\n",
      "test auc: 0.551111\n",
      "loss at iter 147:0.1846\n",
      "train auc: 0.587231\n",
      "test auc: 0.553333\n",
      "loss at iter 148:0.1827\n",
      "train auc: 0.5902\n",
      "test auc: 0.555556\n",
      "loss at iter 149:0.1807\n",
      "train auc: 0.5902\n",
      "test auc: 0.56\n",
      "loss at iter 150:0.1788\n",
      "train auc: 0.592428\n",
      "test auc: 0.562222\n",
      "loss at iter 151:0.1770\n",
      "train auc: 0.595397\n",
      "test auc: 0.562222\n",
      "loss at iter 152:0.1751\n",
      "train auc: 0.597624\n",
      "test auc: 0.564444\n",
      "loss at iter 153:0.1733\n",
      "train auc: 0.598367\n",
      "test auc: 0.564444\n",
      "loss at iter 154:0.1715\n",
      "train auc: 0.602821\n",
      "test auc: 0.573333\n",
      "loss at iter 155:0.1697\n",
      "train auc: 0.605791\n",
      "test auc: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 156:0.1680\n",
      "train auc: 0.60876\n",
      "test auc: 0.584444\n",
      "loss at iter 157:0.1663\n",
      "train auc: 0.613215\n",
      "test auc: 0.584444\n",
      "loss at iter 158:0.1646\n",
      "train auc: 0.618411\n",
      "test auc: 0.586667\n",
      "loss at iter 159:0.1630\n",
      "train auc: 0.619154\n",
      "test auc: 0.586667\n",
      "loss at iter 160:0.1613\n",
      "train auc: 0.621381\n",
      "test auc: 0.586667\n",
      "loss at iter 161:0.1597\n",
      "train auc: 0.625835\n",
      "test auc: 0.593333\n",
      "loss at iter 162:0.1582\n",
      "train auc: 0.628062\n",
      "test auc: 0.6\n",
      "loss at iter 163:0.1566\n",
      "train auc: 0.63029\n",
      "test auc: 0.608889\n",
      "loss at iter 164:0.1551\n",
      "train auc: 0.63029\n",
      "test auc: 0.613333\n",
      "loss at iter 165:0.1536\n",
      "train auc: 0.634744\n",
      "test auc: 0.615556\n",
      "loss at iter 166:0.1521\n",
      "train auc: 0.638456\n",
      "test auc: 0.615556\n",
      "loss at iter 167:0.1507\n",
      "train auc: 0.642168\n",
      "test auc: 0.615556\n",
      "loss at iter 168:0.1492\n",
      "train auc: 0.645137\n",
      "test auc: 0.617778\n",
      "loss at iter 169:0.1478\n",
      "train auc: 0.64588\n",
      "test auc: 0.624444\n",
      "loss at iter 170:0.1464\n",
      "train auc: 0.651076\n",
      "test auc: 0.626667\n",
      "loss at iter 171:0.1451\n",
      "train auc: 0.653304\n",
      "test auc: 0.626667\n",
      "loss at iter 172:0.1437\n",
      "train auc: 0.659243\n",
      "test auc: 0.628889\n",
      "loss at iter 173:0.1424\n",
      "train auc: 0.660728\n",
      "test auc: 0.635556\n",
      "loss at iter 174:0.1411\n",
      "train auc: 0.66147\n",
      "test auc: 0.637778\n",
      "loss at iter 175:0.1398\n",
      "train auc: 0.664439\n",
      "test auc: 0.644444\n",
      "loss at iter 176:0.1386\n",
      "train auc: 0.665182\n",
      "test auc: 0.646667\n",
      "loss at iter 177:0.1373\n",
      "train auc: 0.665924\n",
      "test auc: 0.646667\n",
      "loss at iter 178:0.1361\n",
      "train auc: 0.668151\n",
      "test auc: 0.646667\n",
      "loss at iter 179:0.1349\n",
      "train auc: 0.671863\n",
      "test auc: 0.655556\n",
      "loss at iter 180:0.1338\n",
      "train auc: 0.674091\n",
      "test auc: 0.657778\n",
      "loss at iter 181:0.1326\n",
      "train auc: 0.674833\n",
      "test auc: 0.657778\n",
      "loss at iter 182:0.1315\n",
      "train auc: 0.678545\n",
      "test auc: 0.662222\n",
      "loss at iter 183:0.1303\n",
      "train auc: 0.680772\n",
      "test auc: 0.668889\n",
      "loss at iter 184:0.1292\n",
      "train auc: 0.684484\n",
      "test auc: 0.668889\n",
      "loss at iter 185:0.1282\n",
      "train auc: 0.687454\n",
      "test auc: 0.668889\n",
      "loss at iter 186:0.1271\n",
      "train auc: 0.690423\n",
      "test auc: 0.671111\n",
      "loss at iter 187:0.1261\n",
      "train auc: 0.690423\n",
      "test auc: 0.671111\n",
      "loss at iter 188:0.1250\n",
      "train auc: 0.691166\n",
      "test auc: 0.673333\n",
      "loss at iter 189:0.1240\n",
      "train auc: 0.693393\n",
      "test auc: 0.677778\n",
      "loss at iter 190:0.1230\n",
      "train auc: 0.694878\n",
      "test auc: 0.68\n",
      "loss at iter 191:0.1220\n",
      "train auc: 0.697105\n",
      "test auc: 0.68\n",
      "loss at iter 192:0.1211\n",
      "train auc: 0.699332\n",
      "test auc: 0.68\n",
      "loss at iter 193:0.1201\n",
      "train auc: 0.703044\n",
      "test auc: 0.684444\n",
      "loss at iter 194:0.1192\n",
      "train auc: 0.704529\n",
      "test auc: 0.684444\n",
      "loss at iter 195:0.1183\n",
      "train auc: 0.706013\n",
      "test auc: 0.684444\n",
      "loss at iter 196:0.1173\n",
      "train auc: 0.708241\n",
      "test auc: 0.686667\n",
      "loss at iter 197:0.1165\n",
      "train auc: 0.709725\n",
      "test auc: 0.688889\n",
      "loss at iter 198:0.1156\n",
      "train auc: 0.714922\n",
      "test auc: 0.688889\n",
      "loss at iter 199:0.1147\n",
      "train auc: 0.714922\n",
      "test auc: 0.691111\n",
      "loss at iter 200:0.1139\n",
      "train auc: 0.717149\n",
      "test auc: 0.693333\n",
      "loss at iter 201:0.1130\n",
      "train auc: 0.717892\n",
      "test auc: 0.693333\n",
      "loss at iter 202:0.1122\n",
      "train auc: 0.718634\n",
      "test auc: 0.697778\n",
      "loss at iter 203:0.1114\n",
      "train auc: 0.720861\n",
      "test auc: 0.7\n",
      "loss at iter 204:0.1106\n",
      "train auc: 0.723831\n",
      "test auc: 0.704444\n",
      "loss at iter 205:0.1098\n",
      "train auc: 0.726058\n",
      "test auc: 0.706667\n",
      "loss at iter 206:0.1090\n",
      "train auc: 0.72977\n",
      "test auc: 0.708889\n",
      "loss at iter 207:0.1083\n",
      "train auc: 0.731255\n",
      "test auc: 0.717778\n",
      "loss at iter 208:0.1075\n",
      "train auc: 0.731997\n",
      "test auc: 0.722222\n",
      "loss at iter 209:0.1068\n",
      "train auc: 0.733482\n",
      "test auc: 0.722222\n",
      "loss at iter 210:0.1060\n",
      "train auc: 0.734967\n",
      "test auc: 0.724444\n",
      "loss at iter 211:0.1053\n",
      "train auc: 0.737936\n",
      "test auc: 0.724444\n",
      "loss at iter 212:0.1046\n",
      "train auc: 0.737936\n",
      "test auc: 0.724444\n",
      "loss at iter 213:0.1039\n",
      "train auc: 0.740906\n",
      "test auc: 0.722222\n",
      "loss at iter 214:0.1032\n",
      "train auc: 0.740906\n",
      "test auc: 0.722222\n",
      "loss at iter 215:0.1026\n",
      "train auc: 0.742391\n",
      "test auc: 0.726667\n",
      "loss at iter 216:0.1019\n",
      "train auc: 0.743133\n",
      "test auc: 0.726667\n",
      "loss at iter 217:0.1012\n",
      "train auc: 0.743133\n",
      "test auc: 0.728889\n",
      "loss at iter 218:0.1006\n",
      "train auc: 0.743875\n",
      "test auc: 0.731111\n",
      "loss at iter 219:0.0999\n",
      "train auc: 0.74536\n",
      "test auc: 0.733333\n",
      "loss at iter 220:0.0993\n",
      "train auc: 0.746845\n",
      "test auc: 0.733333\n",
      "loss at iter 221:0.0987\n",
      "train auc: 0.74833\n",
      "test auc: 0.737778\n",
      "loss at iter 222:0.0980\n",
      "train auc: 0.749814\n",
      "test auc: 0.737778\n",
      "loss at iter 223:0.0974\n",
      "train auc: 0.754269\n",
      "test auc: 0.742222\n",
      "loss at iter 224:0.0968\n",
      "train auc: 0.754269\n",
      "test auc: 0.742222\n",
      "loss at iter 225:0.0962\n",
      "train auc: 0.755011\n",
      "test auc: 0.742222\n",
      "loss at iter 226:0.0957\n",
      "train auc: 0.755754\n",
      "test auc: 0.742222\n",
      "loss at iter 227:0.0951\n",
      "train auc: 0.756496\n",
      "test auc: 0.744444\n",
      "loss at iter 228:0.0945\n",
      "train auc: 0.758723\n",
      "test auc: 0.744444\n",
      "loss at iter 229:0.0939\n",
      "train auc: 0.759465\n",
      "test auc: 0.744444\n",
      "loss at iter 230:0.0934\n",
      "train auc: 0.761693\n",
      "test auc: 0.746667\n",
      "loss at iter 231:0.0928\n",
      "train auc: 0.761693\n",
      "test auc: 0.748889\n",
      "loss at iter 232:0.0923\n",
      "train auc: 0.76392\n",
      "test auc: 0.748889\n",
      "loss at iter 233:0.0917\n",
      "train auc: 0.765405\n",
      "test auc: 0.748889\n",
      "loss at iter 234:0.0912\n",
      "train auc: 0.766889\n",
      "test auc: 0.753333\n",
      "loss at iter 235:0.0907\n",
      "train auc: 0.767632\n",
      "test auc: 0.753333\n",
      "loss at iter 236:0.0902\n",
      "train auc: 0.768374\n",
      "test auc: 0.755556\n",
      "loss at iter 237:0.0897\n",
      "train auc: 0.769117\n",
      "test auc: 0.755556\n",
      "loss at iter 238:0.0892\n",
      "train auc: 0.771344\n",
      "test auc: 0.755556\n",
      "loss at iter 239:0.0886\n",
      "train auc: 0.772829\n",
      "test auc: 0.755556\n",
      "loss at iter 240:0.0882\n",
      "train auc: 0.774313\n",
      "test auc: 0.755556\n",
      "loss at iter 241:0.0877\n",
      "train auc: 0.775798\n",
      "test auc: 0.755556\n",
      "loss at iter 242:0.0872\n",
      "train auc: 0.778025\n",
      "test auc: 0.755556\n",
      "loss at iter 243:0.0867\n",
      "train auc: 0.778025\n",
      "test auc: 0.762222\n",
      "loss at iter 244:0.0862\n",
      "train auc: 0.778025\n",
      "test auc: 0.764444\n",
      "loss at iter 245:0.0857\n",
      "train auc: 0.778768\n",
      "test auc: 0.764444\n",
      "loss at iter 246:0.0853\n",
      "train auc: 0.77951\n",
      "test auc: 0.766667\n",
      "loss at iter 247:0.0848\n",
      "train auc: 0.780995\n",
      "test auc: 0.766667\n",
      "loss at iter 248:0.0844\n",
      "train auc: 0.780995\n",
      "test auc: 0.766667\n",
      "loss at iter 249:0.0839\n",
      "train auc: 0.781737\n",
      "test auc: 0.766667\n",
      "loss at iter 250:0.0835\n",
      "train auc: 0.78248\n",
      "test auc: 0.766667\n",
      "loss at iter 251:0.0830\n",
      "train auc: 0.781737\n",
      "test auc: 0.766667\n",
      "loss at iter 252:0.0826\n",
      "train auc: 0.781737\n",
      "test auc: 0.766667\n",
      "loss at iter 253:0.0821\n",
      "train auc: 0.783222\n",
      "test auc: 0.766667\n",
      "loss at iter 254:0.0817\n",
      "train auc: 0.784707\n",
      "test auc: 0.766667\n",
      "loss at iter 255:0.0813\n",
      "train auc: 0.785449\n",
      "test auc: 0.766667\n",
      "loss at iter 256:0.0809\n",
      "train auc: 0.786192\n",
      "test auc: 0.766667\n",
      "loss at iter 257:0.0804\n",
      "train auc: 0.786192\n",
      "test auc: 0.766667\n",
      "loss at iter 258:0.0800\n",
      "train auc: 0.786192\n",
      "test auc: 0.766667\n",
      "loss at iter 259:0.0796\n",
      "train auc: 0.786934\n",
      "test auc: 0.766667\n",
      "loss at iter 260:0.0792\n",
      "train auc: 0.786934\n",
      "test auc: 0.766667\n",
      "loss at iter 261:0.0788\n",
      "train auc: 0.787676\n",
      "test auc: 0.768889\n",
      "loss at iter 262:0.0784\n",
      "train auc: 0.789161\n",
      "test auc: 0.768889\n",
      "loss at iter 263:0.0780\n",
      "train auc: 0.789903\n",
      "test auc: 0.768889\n",
      "loss at iter 264:0.0776\n",
      "train auc: 0.790646\n",
      "test auc: 0.771111\n",
      "loss at iter 265:0.0772\n",
      "train auc: 0.792131\n",
      "test auc: 0.771111\n",
      "loss at iter 266:0.0769\n",
      "train auc: 0.792873\n",
      "test auc: 0.771111\n",
      "loss at iter 267:0.0765\n",
      "train auc: 0.794358\n",
      "test auc: 0.771111\n",
      "loss at iter 268:0.0761\n",
      "train auc: 0.796585\n",
      "test auc: 0.771111\n",
      "loss at iter 269:0.0757\n",
      "train auc: 0.798812\n",
      "test auc: 0.773333\n",
      "loss at iter 270:0.0753\n",
      "train auc: 0.800297\n",
      "test auc: 0.773333\n",
      "loss at iter 271:0.0750\n",
      "train auc: 0.801782\n",
      "test auc: 0.775556\n",
      "loss at iter 272:0.0746\n",
      "train auc: 0.802524\n",
      "test auc: 0.775556\n",
      "loss at iter 273:0.0742\n",
      "train auc: 0.802524\n",
      "test auc: 0.78\n",
      "loss at iter 274:0.0739\n",
      "train auc: 0.803267\n",
      "test auc: 0.782222\n",
      "loss at iter 275:0.0735\n",
      "train auc: 0.804009\n",
      "test auc: 0.782222\n",
      "loss at iter 276:0.0732\n",
      "train auc: 0.804751\n",
      "test auc: 0.784444\n",
      "loss at iter 277:0.0728\n",
      "train auc: 0.806236\n",
      "test auc: 0.784444\n",
      "loss at iter 278:0.0725\n",
      "train auc: 0.807721\n",
      "test auc: 0.786667\n",
      "loss at iter 279:0.0721\n",
      "train auc: 0.808463\n",
      "test auc: 0.786667\n",
      "loss at iter 280:0.0718\n",
      "train auc: 0.81069\n",
      "test auc: 0.786667\n",
      "loss at iter 281:0.0714\n",
      "train auc: 0.81069\n",
      "test auc: 0.786667\n",
      "loss at iter 282:0.0711\n",
      "train auc: 0.81069\n",
      "test auc: 0.786667\n",
      "loss at iter 283:0.0708\n",
      "train auc: 0.811433\n",
      "test auc: 0.788889\n",
      "loss at iter 284:0.0704\n",
      "train auc: 0.812918\n",
      "test auc: 0.788889\n",
      "loss at iter 285:0.0701\n",
      "train auc: 0.812918\n",
      "test auc: 0.788889\n",
      "loss at iter 286:0.0698\n",
      "train auc: 0.815145\n",
      "test auc: 0.788889\n",
      "loss at iter 287:0.0694\n",
      "train auc: 0.815145\n",
      "test auc: 0.788889\n",
      "loss at iter 288:0.0691\n",
      "train auc: 0.815145\n",
      "test auc: 0.788889\n",
      "loss at iter 289:0.0688\n",
      "train auc: 0.815145\n",
      "test auc: 0.788889\n",
      "loss at iter 290:0.0685\n",
      "train auc: 0.815145\n",
      "test auc: 0.788889\n",
      "loss at iter 291:0.0682\n",
      "train auc: 0.815887\n",
      "test auc: 0.788889\n",
      "loss at iter 292:0.0679\n",
      "train auc: 0.815887\n",
      "test auc: 0.788889\n",
      "loss at iter 293:0.0675\n",
      "train auc: 0.817372\n",
      "test auc: 0.791111\n",
      "loss at iter 294:0.0672\n",
      "train auc: 0.818857\n",
      "test auc: 0.791111\n",
      "loss at iter 295:0.0669\n",
      "train auc: 0.819599\n",
      "test auc: 0.791111\n",
      "loss at iter 296:0.0666\n",
      "train auc: 0.821084\n",
      "test auc: 0.791111\n",
      "loss at iter 297:0.0663\n",
      "train auc: 0.822569\n",
      "test auc: 0.791111\n",
      "loss at iter 298:0.0660\n",
      "train auc: 0.824053\n",
      "test auc: 0.791111\n",
      "loss at iter 299:0.0657\n",
      "train auc: 0.825538\n",
      "test auc: 0.791111\n",
      "loss at iter 300:0.0654\n",
      "train auc: 0.826281\n",
      "test auc: 0.791111\n",
      "loss at iter 301:0.0651\n",
      "train auc: 0.827023\n",
      "test auc: 0.791111\n",
      "loss at iter 302:0.0648\n",
      "train auc: 0.829993\n",
      "test auc: 0.791111\n",
      "loss at iter 303:0.0645\n",
      "train auc: 0.830735\n",
      "test auc: 0.793333\n",
      "loss at iter 304:0.0643\n",
      "train auc: 0.830735\n",
      "test auc: 0.795556\n",
      "loss at iter 305:0.0640\n",
      "train auc: 0.830735\n",
      "test auc: 0.795556\n",
      "loss at iter 306:0.0637\n",
      "train auc: 0.831477\n",
      "test auc: 0.795556\n",
      "loss at iter 307:0.0634\n",
      "train auc: 0.832962\n",
      "test auc: 0.795556\n",
      "loss at iter 308:0.0631\n",
      "train auc: 0.833705\n",
      "test auc: 0.795556\n",
      "loss at iter 309:0.0628\n",
      "train auc: 0.833705\n",
      "test auc: 0.797778\n",
      "loss at iter 310:0.0626\n",
      "train auc: 0.834447\n",
      "test auc: 0.797778\n",
      "loss at iter 311:0.0623\n",
      "train auc: 0.835189\n",
      "test auc: 0.797778\n",
      "loss at iter 312:0.0620\n",
      "train auc: 0.836674\n",
      "test auc: 0.797778\n",
      "loss at iter 313:0.0617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.836674\n",
      "test auc: 0.802222\n",
      "loss at iter 314:0.0615\n",
      "train auc: 0.837416\n",
      "test auc: 0.802222\n",
      "loss at iter 315:0.0612\n",
      "train auc: 0.837416\n",
      "test auc: 0.804444\n",
      "loss at iter 316:0.0609\n",
      "train auc: 0.838159\n",
      "test auc: 0.804444\n",
      "loss at iter 317:0.0607\n",
      "train auc: 0.838901\n",
      "test auc: 0.808889\n",
      "loss at iter 318:0.0604\n",
      "train auc: 0.838901\n",
      "test auc: 0.808889\n",
      "loss at iter 319:0.0602\n",
      "train auc: 0.838901\n",
      "test auc: 0.808889\n",
      "loss at iter 320:0.0599\n",
      "train auc: 0.838901\n",
      "test auc: 0.811111\n",
      "loss at iter 321:0.0596\n",
      "train auc: 0.838159\n",
      "test auc: 0.811111\n",
      "loss at iter 322:0.0594\n",
      "train auc: 0.838901\n",
      "test auc: 0.811111\n",
      "loss at iter 323:0.0591\n",
      "train auc: 0.839644\n",
      "test auc: 0.813333\n",
      "loss at iter 324:0.0589\n",
      "train auc: 0.841128\n",
      "test auc: 0.813333\n",
      "loss at iter 325:0.0586\n",
      "train auc: 0.841128\n",
      "test auc: 0.811111\n",
      "loss at iter 326:0.0584\n",
      "train auc: 0.841871\n",
      "test auc: 0.811111\n",
      "loss at iter 327:0.0581\n",
      "train auc: 0.843356\n",
      "test auc: 0.811111\n",
      "loss at iter 328:0.0579\n",
      "train auc: 0.843356\n",
      "test auc: 0.813333\n",
      "loss at iter 329:0.0576\n",
      "train auc: 0.84484\n",
      "test auc: 0.813333\n",
      "loss at iter 330:0.0574\n",
      "train auc: 0.845583\n",
      "test auc: 0.815556\n",
      "loss at iter 331:0.0571\n",
      "train auc: 0.846325\n",
      "test auc: 0.815556\n",
      "loss at iter 332:0.0569\n",
      "train auc: 0.847068\n",
      "test auc: 0.815556\n",
      "loss at iter 333:0.0567\n",
      "train auc: 0.84781\n",
      "test auc: 0.815556\n",
      "loss at iter 334:0.0564\n",
      "train auc: 0.84781\n",
      "test auc: 0.815556\n",
      "loss at iter 335:0.0562\n",
      "train auc: 0.848552\n",
      "test auc: 0.815556\n",
      "loss at iter 336:0.0559\n",
      "train auc: 0.848552\n",
      "test auc: 0.815556\n",
      "loss at iter 337:0.0557\n",
      "train auc: 0.849295\n",
      "test auc: 0.815556\n",
      "loss at iter 338:0.0555\n",
      "train auc: 0.849295\n",
      "test auc: 0.815556\n",
      "loss at iter 339:0.0552\n",
      "train auc: 0.849295\n",
      "test auc: 0.817778\n",
      "loss at iter 340:0.0550\n",
      "train auc: 0.850037\n",
      "test auc: 0.817778\n",
      "loss at iter 341:0.0548\n",
      "train auc: 0.850037\n",
      "test auc: 0.817778\n",
      "loss at iter 342:0.0546\n",
      "train auc: 0.851522\n",
      "test auc: 0.817778\n",
      "loss at iter 343:0.0543\n",
      "train auc: 0.852264\n",
      "test auc: 0.817778\n",
      "loss at iter 344:0.0541\n",
      "train auc: 0.852264\n",
      "test auc: 0.817778\n",
      "loss at iter 345:0.0539\n",
      "train auc: 0.852264\n",
      "test auc: 0.817778\n",
      "loss at iter 346:0.0537\n",
      "train auc: 0.852264\n",
      "test auc: 0.817778\n",
      "loss at iter 347:0.0534\n",
      "train auc: 0.852264\n",
      "test auc: 0.82\n",
      "loss at iter 348:0.0532\n",
      "train auc: 0.854491\n",
      "test auc: 0.82\n",
      "loss at iter 349:0.0530\n",
      "train auc: 0.854491\n",
      "test auc: 0.826667\n",
      "loss at iter 350:0.0528\n",
      "train auc: 0.854491\n",
      "test auc: 0.828889\n",
      "loss at iter 351:0.0526\n",
      "train auc: 0.855234\n",
      "test auc: 0.828889\n",
      "loss at iter 352:0.0524\n",
      "train auc: 0.855976\n",
      "test auc: 0.828889\n",
      "loss at iter 353:0.0521\n",
      "train auc: 0.855976\n",
      "test auc: 0.828889\n",
      "loss at iter 354:0.0519\n",
      "train auc: 0.856719\n",
      "test auc: 0.828889\n",
      "loss at iter 355:0.0517\n",
      "train auc: 0.856719\n",
      "test auc: 0.828889\n",
      "loss at iter 356:0.0515\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 357:0.0513\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 358:0.0511\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 359:0.0509\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 360:0.0507\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 361:0.0505\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 362:0.0503\n",
      "train auc: 0.857461\n",
      "test auc: 0.828889\n",
      "loss at iter 363:0.0501\n",
      "train auc: 0.858203\n",
      "test auc: 0.828889\n",
      "loss at iter 364:0.0499\n",
      "train auc: 0.859688\n",
      "test auc: 0.828889\n",
      "loss at iter 365:0.0497\n",
      "train auc: 0.859688\n",
      "test auc: 0.828889\n",
      "loss at iter 366:0.0495\n",
      "train auc: 0.859688\n",
      "test auc: 0.828889\n",
      "loss at iter 367:0.0493\n",
      "train auc: 0.859688\n",
      "test auc: 0.828889\n",
      "loss at iter 368:0.0491\n",
      "train auc: 0.860431\n",
      "test auc: 0.828889\n",
      "loss at iter 369:0.0489\n",
      "train auc: 0.861173\n",
      "test auc: 0.828889\n",
      "loss at iter 370:0.0487\n",
      "train auc: 0.861173\n",
      "test auc: 0.828889\n",
      "loss at iter 371:0.0485\n",
      "train auc: 0.861173\n",
      "test auc: 0.828889\n",
      "loss at iter 372:0.0483\n",
      "train auc: 0.861915\n",
      "test auc: 0.833333\n",
      "loss at iter 373:0.0481\n",
      "train auc: 0.861915\n",
      "test auc: 0.833333\n",
      "loss at iter 374:0.0479\n",
      "train auc: 0.862658\n",
      "test auc: 0.833333\n",
      "loss at iter 375:0.0477\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 376:0.0475\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 377:0.0473\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 378:0.0472\n",
      "train auc: 0.862658\n",
      "test auc: 0.833333\n",
      "loss at iter 379:0.0470\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 380:0.0468\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 381:0.0466\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 382:0.0464\n",
      "train auc: 0.8634\n",
      "test auc: 0.833333\n",
      "loss at iter 383:0.0462\n",
      "train auc: 0.864143\n",
      "test auc: 0.833333\n",
      "loss at iter 384:0.0461\n",
      "train auc: 0.864143\n",
      "test auc: 0.835556\n",
      "loss at iter 385:0.0459\n",
      "train auc: 0.864885\n",
      "test auc: 0.837778\n",
      "loss at iter 386:0.0457\n",
      "train auc: 0.864885\n",
      "test auc: 0.837778\n",
      "loss at iter 387:0.0455\n",
      "train auc: 0.865627\n",
      "test auc: 0.84\n",
      "loss at iter 388:0.0453\n",
      "train auc: 0.867112\n",
      "test auc: 0.842222\n",
      "loss at iter 389:0.0452\n",
      "train auc: 0.867112\n",
      "test auc: 0.842222\n",
      "loss at iter 390:0.0450\n",
      "train auc: 0.867112\n",
      "test auc: 0.844444\n",
      "loss at iter 391:0.0448\n",
      "train auc: 0.867854\n",
      "test auc: 0.844444\n",
      "loss at iter 392:0.0446\n",
      "train auc: 0.867854\n",
      "test auc: 0.844444\n",
      "loss at iter 393:0.0445\n",
      "train auc: 0.868597\n",
      "test auc: 0.844444\n",
      "loss at iter 394:0.0443\n",
      "train auc: 0.869339\n",
      "test auc: 0.844444\n",
      "loss at iter 395:0.0441\n",
      "train auc: 0.869339\n",
      "test auc: 0.844444\n",
      "loss at iter 396:0.0440\n",
      "train auc: 0.869339\n",
      "test auc: 0.844444\n",
      "loss at iter 397:0.0438\n",
      "train auc: 0.870082\n",
      "test auc: 0.846667\n",
      "loss at iter 398:0.0436\n",
      "train auc: 0.870824\n",
      "test auc: 0.846667\n",
      "loss at iter 399:0.0434\n",
      "train auc: 0.870824\n",
      "test auc: 0.846667\n",
      "loss at iter 400:0.0433\n",
      "train auc: 0.870824\n",
      "test auc: 0.846667\n",
      "loss at iter 401:0.0431\n",
      "train auc: 0.870824\n",
      "test auc: 0.846667\n",
      "loss at iter 402:0.0430\n",
      "train auc: 0.870824\n",
      "test auc: 0.846667\n",
      "loss at iter 403:0.0428\n",
      "train auc: 0.870824\n",
      "test auc: 0.848889\n",
      "loss at iter 404:0.0426\n",
      "train auc: 0.870824\n",
      "test auc: 0.848889\n",
      "loss at iter 405:0.0425\n",
      "train auc: 0.871566\n",
      "test auc: 0.848889\n",
      "loss at iter 406:0.0423\n",
      "train auc: 0.871566\n",
      "test auc: 0.848889\n",
      "loss at iter 407:0.0421\n",
      "train auc: 0.872309\n",
      "test auc: 0.848889\n",
      "loss at iter 408:0.0420\n",
      "train auc: 0.873051\n",
      "test auc: 0.851111\n",
      "loss at iter 409:0.0418\n",
      "train auc: 0.873051\n",
      "test auc: 0.851111\n",
      "loss at iter 410:0.0417\n",
      "train auc: 0.873051\n",
      "test auc: 0.851111\n",
      "loss at iter 411:0.0415\n",
      "train auc: 0.873051\n",
      "test auc: 0.851111\n",
      "loss at iter 412:0.0413\n",
      "train auc: 0.873794\n",
      "test auc: 0.851111\n",
      "loss at iter 413:0.0412\n",
      "train auc: 0.873794\n",
      "test auc: 0.851111\n",
      "loss at iter 414:0.0410\n",
      "train auc: 0.873794\n",
      "test auc: 0.851111\n",
      "loss at iter 415:0.0409\n",
      "train auc: 0.874536\n",
      "test auc: 0.851111\n",
      "loss at iter 416:0.0407\n",
      "train auc: 0.875278\n",
      "test auc: 0.853333\n",
      "loss at iter 417:0.0406\n",
      "train auc: 0.875278\n",
      "test auc: 0.853333\n",
      "loss at iter 418:0.0404\n",
      "train auc: 0.875278\n",
      "test auc: 0.853333\n",
      "loss at iter 419:0.0403\n",
      "train auc: 0.875278\n",
      "test auc: 0.855556\n",
      "loss at iter 420:0.0401\n",
      "train auc: 0.875278\n",
      "test auc: 0.855556\n",
      "loss at iter 421:0.0400\n",
      "train auc: 0.876021\n",
      "test auc: 0.855556\n",
      "loss at iter 422:0.0398\n",
      "train auc: 0.876763\n",
      "test auc: 0.855556\n",
      "loss at iter 423:0.0397\n",
      "train auc: 0.877506\n",
      "test auc: 0.855556\n",
      "loss at iter 424:0.0395\n",
      "train auc: 0.877506\n",
      "test auc: 0.857778\n",
      "loss at iter 425:0.0394\n",
      "train auc: 0.87899\n",
      "test auc: 0.857778\n",
      "loss at iter 426:0.0392\n",
      "train auc: 0.879733\n",
      "test auc: 0.857778\n",
      "loss at iter 427:0.0391\n",
      "train auc: 0.879733\n",
      "test auc: 0.857778\n",
      "loss at iter 428:0.0390\n",
      "train auc: 0.880475\n",
      "test auc: 0.857778\n",
      "loss at iter 429:0.0388\n",
      "train auc: 0.880475\n",
      "test auc: 0.857778\n",
      "loss at iter 430:0.0387\n",
      "train auc: 0.88196\n",
      "test auc: 0.857778\n",
      "loss at iter 431:0.0385\n",
      "train auc: 0.88196\n",
      "test auc: 0.857778\n",
      "loss at iter 432:0.0384\n",
      "train auc: 0.88196\n",
      "test auc: 0.857778\n",
      "loss at iter 433:0.0382\n",
      "train auc: 0.88196\n",
      "test auc: 0.857778\n",
      "loss at iter 434:0.0381\n",
      "train auc: 0.882702\n",
      "test auc: 0.86\n",
      "loss at iter 435:0.0380\n",
      "train auc: 0.882702\n",
      "test auc: 0.86\n",
      "loss at iter 436:0.0378\n",
      "train auc: 0.882702\n",
      "test auc: 0.86\n",
      "loss at iter 437:0.0377\n",
      "train auc: 0.883445\n",
      "test auc: 0.86\n",
      "loss at iter 438:0.0376\n",
      "train auc: 0.883445\n",
      "test auc: 0.86\n",
      "loss at iter 439:0.0374\n",
      "train auc: 0.883445\n",
      "test auc: 0.86\n",
      "loss at iter 440:0.0373\n",
      "train auc: 0.883445\n",
      "test auc: 0.86\n",
      "loss at iter 441:0.0371\n",
      "train auc: 0.883445\n",
      "test auc: 0.862222\n",
      "loss at iter 442:0.0370\n",
      "train auc: 0.883445\n",
      "test auc: 0.862222\n",
      "loss at iter 443:0.0369\n",
      "train auc: 0.883445\n",
      "test auc: 0.862222\n",
      "loss at iter 444:0.0367\n",
      "train auc: 0.883445\n",
      "test auc: 0.862222\n",
      "loss at iter 445:0.0366\n",
      "train auc: 0.884187\n",
      "test auc: 0.862222\n",
      "loss at iter 446:0.0365\n",
      "train auc: 0.884929\n",
      "test auc: 0.866667\n",
      "loss at iter 447:0.0364\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 448:0.0362\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 449:0.0361\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 450:0.0360\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 451:0.0358\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 452:0.0357\n",
      "train auc: 0.885672\n",
      "test auc: 0.866667\n",
      "loss at iter 453:0.0356\n",
      "train auc: 0.887899\n",
      "test auc: 0.866667\n",
      "loss at iter 454:0.0355\n",
      "train auc: 0.887899\n",
      "test auc: 0.866667\n",
      "loss at iter 455:0.0353\n",
      "train auc: 0.887899\n",
      "test auc: 0.868889\n",
      "loss at iter 456:0.0352\n",
      "train auc: 0.888641\n",
      "test auc: 0.868889\n",
      "loss at iter 457:0.0351\n",
      "train auc: 0.888641\n",
      "test auc: 0.868889\n",
      "loss at iter 458:0.0350\n",
      "train auc: 0.890126\n",
      "test auc: 0.868889\n",
      "loss at iter 459:0.0348\n",
      "train auc: 0.890869\n",
      "test auc: 0.868889\n",
      "loss at iter 460:0.0347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 461:0.0346\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 462:0.0345\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 463:0.0344\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 464:0.0342\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 465:0.0341\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 466:0.0340\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 467:0.0339\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 468:0.0338\n",
      "train auc: 0.891611\n",
      "test auc: 0.868889\n",
      "loss at iter 469:0.0336\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 470:0.0335\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 471:0.0334\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 472:0.0333\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 473:0.0332\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 474:0.0331\n",
      "train auc: 0.892353\n",
      "test auc: 0.868889\n",
      "loss at iter 475:0.0330\n",
      "train auc: 0.893096\n",
      "test auc: 0.868889\n",
      "loss at iter 476:0.0328\n",
      "train auc: 0.893838\n",
      "test auc: 0.868889\n",
      "loss at iter 477:0.0327\n",
      "train auc: 0.893838\n",
      "test auc: 0.868889\n",
      "loss at iter 478:0.0326\n",
      "train auc: 0.893838\n",
      "test auc: 0.868889\n",
      "loss at iter 479:0.0325\n",
      "train auc: 0.895323\n",
      "test auc: 0.868889\n",
      "loss at iter 480:0.0324\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 481:0.0323\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 482:0.0322\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 483:0.0321\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 484:0.0320\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 485:0.0319\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 486:0.0318\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 487:0.0317\n",
      "train auc: 0.896808\n",
      "test auc: 0.868889\n",
      "loss at iter 488:0.0315\n",
      "train auc: 0.898292\n",
      "test auc: 0.868889\n",
      "loss at iter 489:0.0314\n",
      "train auc: 0.899035\n",
      "test auc: 0.868889\n",
      "loss at iter 490:0.0313\n",
      "train auc: 0.899035\n",
      "test auc: 0.868889\n",
      "loss at iter 491:0.0312\n",
      "train auc: 0.899035\n",
      "test auc: 0.871111\n",
      "loss at iter 492:0.0311\n",
      "train auc: 0.899777\n",
      "test auc: 0.871111\n",
      "loss at iter 493:0.0310\n",
      "train auc: 0.899777\n",
      "test auc: 0.871111\n",
      "loss at iter 494:0.0309\n",
      "train auc: 0.899777\n",
      "test auc: 0.871111\n",
      "loss at iter 495:0.0308\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 496:0.0307\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 497:0.0306\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 498:0.0305\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 499:0.0304\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 500:0.0303\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 501:0.0302\n",
      "train auc: 0.901262\n",
      "test auc: 0.871111\n",
      "loss at iter 502:0.0301\n",
      "train auc: 0.902747\n",
      "test auc: 0.871111\n",
      "loss at iter 503:0.0300\n",
      "train auc: 0.902747\n",
      "test auc: 0.871111\n",
      "loss at iter 504:0.0299\n",
      "train auc: 0.903489\n",
      "test auc: 0.871111\n",
      "loss at iter 505:0.0298\n",
      "train auc: 0.903489\n",
      "test auc: 0.871111\n",
      "loss at iter 506:0.0297\n",
      "train auc: 0.904232\n",
      "test auc: 0.871111\n",
      "loss at iter 507:0.0296\n",
      "train auc: 0.904974\n",
      "test auc: 0.871111\n",
      "loss at iter 508:0.0295\n",
      "train auc: 0.904974\n",
      "test auc: 0.871111\n",
      "loss at iter 509:0.0294\n",
      "train auc: 0.904974\n",
      "test auc: 0.871111\n",
      "loss at iter 510:0.0294\n",
      "train auc: 0.905716\n",
      "test auc: 0.871111\n",
      "loss at iter 511:0.0293\n",
      "train auc: 0.906459\n",
      "test auc: 0.871111\n",
      "loss at iter 512:0.0292\n",
      "train auc: 0.907944\n",
      "test auc: 0.873333\n",
      "loss at iter 513:0.0291\n",
      "train auc: 0.908686\n",
      "test auc: 0.873333\n",
      "loss at iter 514:0.0290\n",
      "train auc: 0.910171\n",
      "test auc: 0.873333\n",
      "loss at iter 515:0.0289\n",
      "train auc: 0.910171\n",
      "test auc: 0.873333\n",
      "loss at iter 516:0.0288\n",
      "train auc: 0.910913\n",
      "test auc: 0.873333\n",
      "loss at iter 517:0.0287\n",
      "train auc: 0.912398\n",
      "test auc: 0.873333\n",
      "loss at iter 518:0.0286\n",
      "train auc: 0.912398\n",
      "test auc: 0.873333\n",
      "loss at iter 519:0.0285\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 520:0.0284\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 521:0.0283\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 522:0.0283\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 523:0.0282\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 524:0.0281\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 525:0.0280\n",
      "train auc: 0.91314\n",
      "test auc: 0.873333\n",
      "loss at iter 526:0.0279\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 527:0.0278\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 528:0.0277\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 529:0.0276\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 530:0.0276\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 531:0.0275\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 532:0.0274\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 533:0.0273\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 534:0.0272\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 535:0.0271\n",
      "train auc: 0.913883\n",
      "test auc: 0.873333\n",
      "loss at iter 536:0.0271\n",
      "train auc: 0.914625\n",
      "test auc: 0.873333\n",
      "loss at iter 537:0.0270\n",
      "train auc: 0.914625\n",
      "test auc: 0.873333\n",
      "loss at iter 538:0.0269\n",
      "train auc: 0.915367\n",
      "test auc: 0.875556\n",
      "loss at iter 539:0.0268\n",
      "train auc: 0.915367\n",
      "test auc: 0.875556\n",
      "loss at iter 540:0.0267\n",
      "train auc: 0.915367\n",
      "test auc: 0.875556\n",
      "loss at iter 541:0.0266\n",
      "train auc: 0.91611\n",
      "test auc: 0.875556\n",
      "loss at iter 542:0.0266\n",
      "train auc: 0.916852\n",
      "test auc: 0.875556\n",
      "loss at iter 543:0.0265\n",
      "train auc: 0.916852\n",
      "test auc: 0.875556\n",
      "loss at iter 544:0.0264\n",
      "train auc: 0.917595\n",
      "test auc: 0.875556\n",
      "loss at iter 545:0.0263\n",
      "train auc: 0.917595\n",
      "test auc: 0.875556\n",
      "loss at iter 546:0.0262\n",
      "train auc: 0.918337\n",
      "test auc: 0.877778\n",
      "loss at iter 547:0.0262\n",
      "train auc: 0.918337\n",
      "test auc: 0.877778\n",
      "loss at iter 548:0.0261\n",
      "train auc: 0.919079\n",
      "test auc: 0.877778\n",
      "loss at iter 549:0.0260\n",
      "train auc: 0.919079\n",
      "test auc: 0.877778\n",
      "loss at iter 550:0.0259\n",
      "train auc: 0.919822\n",
      "test auc: 0.877778\n",
      "loss at iter 551:0.0259\n",
      "train auc: 0.919822\n",
      "test auc: 0.877778\n",
      "loss at iter 552:0.0258\n",
      "train auc: 0.919822\n",
      "test auc: 0.877778\n",
      "loss at iter 553:0.0257\n",
      "train auc: 0.919822\n",
      "test auc: 0.877778\n",
      "loss at iter 554:0.0256\n",
      "train auc: 0.920564\n",
      "test auc: 0.877778\n",
      "loss at iter 555:0.0256\n",
      "train auc: 0.920564\n",
      "test auc: 0.877778\n",
      "loss at iter 556:0.0255\n",
      "train auc: 0.920564\n",
      "test auc: 0.877778\n",
      "loss at iter 557:0.0254\n",
      "train auc: 0.920564\n",
      "test auc: 0.877778\n",
      "loss at iter 558:0.0253\n",
      "train auc: 0.921307\n",
      "test auc: 0.877778\n",
      "loss at iter 559:0.0253\n",
      "train auc: 0.922049\n",
      "test auc: 0.877778\n",
      "loss at iter 560:0.0252\n",
      "train auc: 0.922049\n",
      "test auc: 0.877778\n",
      "loss at iter 561:0.0251\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 562:0.0250\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 563:0.0250\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 564:0.0249\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 565:0.0248\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 566:0.0248\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 567:0.0247\n",
      "train auc: 0.923534\n",
      "test auc: 0.877778\n",
      "loss at iter 568:0.0246\n",
      "train auc: 0.923534\n",
      "test auc: 0.88\n",
      "loss at iter 569:0.0245\n",
      "train auc: 0.923534\n",
      "test auc: 0.88\n",
      "loss at iter 570:0.0245\n",
      "train auc: 0.923534\n",
      "test auc: 0.88\n",
      "loss at iter 571:0.0244\n",
      "train auc: 0.923534\n",
      "test auc: 0.88\n",
      "loss at iter 572:0.0243\n",
      "train auc: 0.925019\n",
      "test auc: 0.88\n",
      "loss at iter 573:0.0243\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 574:0.0242\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 575:0.0241\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 576:0.0241\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 577:0.0240\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 578:0.0239\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 579:0.0239\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 580:0.0238\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 581:0.0237\n",
      "train auc: 0.926503\n",
      "test auc: 0.88\n",
      "loss at iter 582:0.0237\n",
      "train auc: 0.927988\n",
      "test auc: 0.88\n",
      "loss at iter 583:0.0236\n",
      "train auc: 0.927988\n",
      "test auc: 0.88\n",
      "loss at iter 584:0.0235\n",
      "train auc: 0.92873\n",
      "test auc: 0.88\n",
      "loss at iter 585:0.0235\n",
      "train auc: 0.92873\n",
      "test auc: 0.88\n",
      "loss at iter 586:0.0234\n",
      "train auc: 0.930215\n",
      "test auc: 0.88\n",
      "loss at iter 587:0.0233\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 588:0.0233\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 589:0.0232\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 590:0.0231\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 591:0.0231\n",
      "train auc: 0.930215\n",
      "test auc: 0.88\n",
      "loss at iter 592:0.0230\n",
      "train auc: 0.930215\n",
      "test auc: 0.88\n",
      "loss at iter 593:0.0229\n",
      "train auc: 0.930215\n",
      "test auc: 0.88\n",
      "loss at iter 594:0.0229\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 595:0.0228\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 596:0.0228\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 597:0.0227\n",
      "train auc: 0.930958\n",
      "test auc: 0.88\n",
      "loss at iter 598:0.0226\n",
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 599:0.0226\n",
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 600:0.0225\n",
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 601:0.0225\n",
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 602:0.0224\n",
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 603:0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.9317\n",
      "test auc: 0.88\n",
      "loss at iter 604:0.0223\n",
      "train auc: 0.932442\n",
      "test auc: 0.88\n",
      "loss at iter 605:0.0222\n",
      "train auc: 0.932442\n",
      "test auc: 0.88\n",
      "loss at iter 606:0.0222\n",
      "train auc: 0.932442\n",
      "test auc: 0.88\n",
      "loss at iter 607:0.0221\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 608:0.0220\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 609:0.0220\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 610:0.0219\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 611:0.0219\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 612:0.0218\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 613:0.0218\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 614:0.0217\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 615:0.0216\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 616:0.0216\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 617:0.0215\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 618:0.0215\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 619:0.0214\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 620:0.0214\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 621:0.0213\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 622:0.0213\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 623:0.0212\n",
      "train auc: 0.933185\n",
      "test auc: 0.88\n",
      "loss at iter 624:0.0211\n",
      "train auc: 0.933927\n",
      "test auc: 0.88\n",
      "loss at iter 625:0.0211\n",
      "train auc: 0.933927\n",
      "test auc: 0.88\n",
      "loss at iter 626:0.0210\n",
      "train auc: 0.933927\n",
      "test auc: 0.88\n",
      "loss at iter 627:0.0210\n",
      "train auc: 0.933927\n",
      "test auc: 0.88\n",
      "loss at iter 628:0.0209\n",
      "train auc: 0.933927\n",
      "test auc: 0.88\n",
      "loss at iter 629:0.0209\n",
      "train auc: 0.93467\n",
      "test auc: 0.88\n",
      "loss at iter 630:0.0208\n",
      "train auc: 0.93467\n",
      "test auc: 0.88\n",
      "loss at iter 631:0.0208\n",
      "train auc: 0.93467\n",
      "test auc: 0.88\n",
      "loss at iter 632:0.0207\n",
      "train auc: 0.93467\n",
      "test auc: 0.88\n",
      "loss at iter 633:0.0207\n",
      "train auc: 0.93467\n",
      "test auc: 0.88\n",
      "loss at iter 634:0.0206\n",
      "train auc: 0.935412\n",
      "test auc: 0.88\n",
      "loss at iter 635:0.0206\n",
      "train auc: 0.935412\n",
      "test auc: 0.88\n",
      "loss at iter 636:0.0205\n",
      "train auc: 0.935412\n",
      "test auc: 0.88\n",
      "loss at iter 637:0.0205\n",
      "train auc: 0.935412\n",
      "test auc: 0.884444\n",
      "loss at iter 638:0.0204\n",
      "train auc: 0.935412\n",
      "test auc: 0.884444\n",
      "loss at iter 639:0.0204\n",
      "train auc: 0.935412\n",
      "test auc: 0.884444\n",
      "loss at iter 640:0.0203\n",
      "train auc: 0.936154\n",
      "test auc: 0.886667\n",
      "loss at iter 641:0.0203\n",
      "train auc: 0.936154\n",
      "test auc: 0.886667\n",
      "loss at iter 642:0.0202\n",
      "train auc: 0.936897\n",
      "test auc: 0.886667\n",
      "loss at iter 643:0.0202\n",
      "train auc: 0.936897\n",
      "test auc: 0.886667\n",
      "loss at iter 644:0.0201\n",
      "train auc: 0.936897\n",
      "test auc: 0.886667\n",
      "loss at iter 645:0.0201\n",
      "train auc: 0.936897\n",
      "test auc: 0.886667\n",
      "loss at iter 646:0.0200\n",
      "train auc: 0.938382\n",
      "test auc: 0.886667\n",
      "loss at iter 647:0.0200\n",
      "train auc: 0.938382\n",
      "test auc: 0.886667\n",
      "loss at iter 648:0.0199\n",
      "train auc: 0.938382\n",
      "test auc: 0.886667\n",
      "loss at iter 649:0.0199\n",
      "train auc: 0.938382\n",
      "test auc: 0.886667\n",
      "loss at iter 650:0.0198\n",
      "train auc: 0.938382\n",
      "test auc: 0.886667\n",
      "loss at iter 651:0.0198\n",
      "train auc: 0.939124\n",
      "test auc: 0.886667\n",
      "loss at iter 652:0.0197\n",
      "train auc: 0.939866\n",
      "test auc: 0.886667\n",
      "loss at iter 653:0.0197\n",
      "train auc: 0.939866\n",
      "test auc: 0.886667\n",
      "loss at iter 654:0.0196\n",
      "train auc: 0.939866\n",
      "test auc: 0.888889\n",
      "loss at iter 655:0.0196\n",
      "train auc: 0.939866\n",
      "test auc: 0.888889\n",
      "loss at iter 656:0.0195\n",
      "train auc: 0.939866\n",
      "test auc: 0.888889\n",
      "loss at iter 657:0.0195\n",
      "train auc: 0.939866\n",
      "test auc: 0.888889\n",
      "loss at iter 658:0.0194\n",
      "train auc: 0.939866\n",
      "test auc: 0.888889\n",
      "loss at iter 659:0.0194\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 660:0.0193\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 661:0.0193\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 662:0.0193\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 663:0.0192\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 664:0.0192\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 665:0.0191\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 666:0.0191\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 667:0.0190\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 668:0.0190\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 669:0.0189\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 670:0.0189\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 671:0.0189\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 672:0.0188\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 673:0.0188\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 674:0.0187\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 675:0.0187\n",
      "train auc: 0.940609\n",
      "test auc: 0.888889\n",
      "loss at iter 676:0.0186\n",
      "train auc: 0.941351\n",
      "test auc: 0.888889\n",
      "loss at iter 677:0.0186\n",
      "train auc: 0.941351\n",
      "test auc: 0.888889\n",
      "loss at iter 678:0.0185\n",
      "train auc: 0.941351\n",
      "test auc: 0.888889\n",
      "loss at iter 679:0.0185\n",
      "train auc: 0.942094\n",
      "test auc: 0.888889\n",
      "loss at iter 680:0.0185\n",
      "train auc: 0.943578\n",
      "test auc: 0.888889\n",
      "loss at iter 681:0.0184\n",
      "train auc: 0.943578\n",
      "test auc: 0.888889\n",
      "loss at iter 682:0.0184\n",
      "train auc: 0.943578\n",
      "test auc: 0.888889\n",
      "loss at iter 683:0.0183\n",
      "train auc: 0.944321\n",
      "test auc: 0.888889\n",
      "loss at iter 684:0.0183\n",
      "train auc: 0.944321\n",
      "test auc: 0.888889\n",
      "loss at iter 685:0.0183\n",
      "train auc: 0.945063\n",
      "test auc: 0.888889\n",
      "loss at iter 686:0.0182\n",
      "train auc: 0.945063\n",
      "test auc: 0.888889\n",
      "loss at iter 687:0.0182\n",
      "train auc: 0.945063\n",
      "test auc: 0.891111\n",
      "loss at iter 688:0.0181\n",
      "train auc: 0.945063\n",
      "test auc: 0.891111\n",
      "loss at iter 689:0.0181\n",
      "train auc: 0.945063\n",
      "test auc: 0.891111\n",
      "loss at iter 690:0.0181\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 691:0.0180\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 692:0.0180\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 693:0.0179\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 694:0.0179\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 695:0.0179\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 696:0.0178\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 697:0.0178\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 698:0.0177\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 699:0.0177\n",
      "train auc: 0.945805\n",
      "test auc: 0.891111\n",
      "loss at iter 700:0.0177\n",
      "train auc: 0.946548\n",
      "test auc: 0.891111\n",
      "loss at iter 701:0.0176\n",
      "train auc: 0.946548\n",
      "test auc: 0.891111\n",
      "loss at iter 702:0.0176\n",
      "train auc: 0.946548\n",
      "test auc: 0.891111\n",
      "loss at iter 703:0.0175\n",
      "train auc: 0.94729\n",
      "test auc: 0.891111\n",
      "loss at iter 704:0.0175\n",
      "train auc: 0.948033\n",
      "test auc: 0.891111\n",
      "loss at iter 705:0.0175\n",
      "train auc: 0.948033\n",
      "test auc: 0.891111\n",
      "loss at iter 706:0.0174\n",
      "train auc: 0.948033\n",
      "test auc: 0.891111\n",
      "loss at iter 707:0.0174\n",
      "train auc: 0.948033\n",
      "test auc: 0.891111\n",
      "loss at iter 708:0.0174\n",
      "train auc: 0.948033\n",
      "test auc: 0.891111\n",
      "loss at iter 709:0.0173\n",
      "train auc: 0.948775\n",
      "test auc: 0.891111\n",
      "loss at iter 710:0.0173\n",
      "train auc: 0.948775\n",
      "test auc: 0.891111\n",
      "loss at iter 711:0.0172\n",
      "train auc: 0.948775\n",
      "test auc: 0.891111\n",
      "loss at iter 712:0.0172\n",
      "train auc: 0.948775\n",
      "test auc: 0.891111\n",
      "loss at iter 713:0.0172\n",
      "train auc: 0.948775\n",
      "test auc: 0.891111\n",
      "loss at iter 714:0.0171\n",
      "train auc: 0.949517\n",
      "test auc: 0.891111\n",
      "loss at iter 715:0.0171\n",
      "train auc: 0.949517\n",
      "test auc: 0.891111\n",
      "loss at iter 716:0.0171\n",
      "train auc: 0.95026\n",
      "test auc: 0.891111\n",
      "loss at iter 717:0.0170\n",
      "train auc: 0.951745\n",
      "test auc: 0.891111\n",
      "loss at iter 718:0.0170\n",
      "train auc: 0.951745\n",
      "test auc: 0.891111\n",
      "loss at iter 719:0.0169\n",
      "train auc: 0.951745\n",
      "test auc: 0.891111\n",
      "loss at iter 720:0.0169\n",
      "train auc: 0.951745\n",
      "test auc: 0.891111\n",
      "loss at iter 721:0.0169\n",
      "train auc: 0.951745\n",
      "test auc: 0.891111\n",
      "loss at iter 722:0.0168\n",
      "train auc: 0.951745\n",
      "test auc: 0.893333\n",
      "loss at iter 723:0.0168\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 724:0.0168\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 725:0.0167\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 726:0.0167\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 727:0.0167\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 728:0.0166\n",
      "train auc: 0.952487\n",
      "test auc: 0.893333\n",
      "loss at iter 729:0.0166\n",
      "train auc: 0.953229\n",
      "test auc: 0.893333\n",
      "loss at iter 730:0.0166\n",
      "train auc: 0.953229\n",
      "test auc: 0.893333\n",
      "loss at iter 731:0.0165\n",
      "train auc: 0.953229\n",
      "test auc: 0.893333\n",
      "loss at iter 732:0.0165\n",
      "train auc: 0.953229\n",
      "test auc: 0.893333\n",
      "loss at iter 733:0.0165\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 734:0.0164\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 735:0.0164\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 736:0.0164\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 737:0.0163\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 738:0.0163\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 739:0.0163\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 740:0.0162\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 741:0.0162\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 742:0.0162\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 743:0.0161\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 744:0.0161\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 745:0.0161\n",
      "train auc: 0.953972\n",
      "test auc: 0.893333\n",
      "loss at iter 746:0.0160\n",
      "train auc: 0.954714\n",
      "test auc: 0.893333\n",
      "loss at iter 747:0.0160\n",
      "train auc: 0.954714\n",
      "test auc: 0.893333\n",
      "loss at iter 748:0.0160\n",
      "train auc: 0.954714\n",
      "test auc: 0.893333\n",
      "loss at iter 749:0.0159\n",
      "train auc: 0.954714\n",
      "test auc: 0.893333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 750:0.0159\n",
      "train auc: 0.955457\n",
      "test auc: 0.893333\n",
      "loss at iter 751:0.0159\n",
      "train auc: 0.955457\n",
      "test auc: 0.893333\n",
      "loss at iter 752:0.0158\n",
      "train auc: 0.956199\n",
      "test auc: 0.893333\n",
      "loss at iter 753:0.0158\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 754:0.0158\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 755:0.0157\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 756:0.0157\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 757:0.0157\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 758:0.0156\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 759:0.0156\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 760:0.0156\n",
      "train auc: 0.957684\n",
      "test auc: 0.893333\n",
      "loss at iter 761:0.0156\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 762:0.0155\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 763:0.0155\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 764:0.0155\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 765:0.0154\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 766:0.0154\n",
      "train auc: 0.958426\n",
      "test auc: 0.893333\n",
      "loss at iter 767:0.0154\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 768:0.0153\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 769:0.0153\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 770:0.0153\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 771:0.0153\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 772:0.0152\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 773:0.0152\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 774:0.0152\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 775:0.0151\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 776:0.0151\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 777:0.0151\n",
      "train auc: 0.958426\n",
      "test auc: 0.895556\n",
      "loss at iter 778:0.0151\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 779:0.0150\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 780:0.0150\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 781:0.0150\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 782:0.0149\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 783:0.0149\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 784:0.0149\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 785:0.0149\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 786:0.0148\n",
      "train auc: 0.958426\n",
      "test auc: 0.897778\n",
      "loss at iter 787:0.0148\n",
      "train auc: 0.959168\n",
      "test auc: 0.897778\n",
      "loss at iter 788:0.0148\n",
      "train auc: 0.959911\n",
      "test auc: 0.897778\n",
      "loss at iter 789:0.0147\n",
      "train auc: 0.960653\n",
      "test auc: 0.897778\n",
      "loss at iter 790:0.0147\n",
      "train auc: 0.960653\n",
      "test auc: 0.897778\n",
      "loss at iter 791:0.0147\n",
      "train auc: 0.960653\n",
      "test auc: 0.897778\n",
      "loss at iter 792:0.0147\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 793:0.0146\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 794:0.0146\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 795:0.0146\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 796:0.0146\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 797:0.0145\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 798:0.0145\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 799:0.0145\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 800:0.0144\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 801:0.0144\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 802:0.0144\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 803:0.0144\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 804:0.0143\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 805:0.0143\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 806:0.0143\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 807:0.0143\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 808:0.0142\n",
      "train auc: 0.961396\n",
      "test auc: 0.897778\n",
      "loss at iter 809:0.0142\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 810:0.0142\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 811:0.0142\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 812:0.0141\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 813:0.0141\n",
      "train auc: 0.962138\n",
      "test auc: 0.895556\n",
      "loss at iter 814:0.0141\n",
      "train auc: 0.962138\n",
      "test auc: 0.895556\n",
      "loss at iter 815:0.0141\n",
      "train auc: 0.962138\n",
      "test auc: 0.895556\n",
      "loss at iter 816:0.0140\n",
      "train auc: 0.962138\n",
      "test auc: 0.895556\n",
      "loss at iter 817:0.0140\n",
      "train auc: 0.962138\n",
      "test auc: 0.895556\n",
      "loss at iter 818:0.0140\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 819:0.0140\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 820:0.0139\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 821:0.0139\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 822:0.0139\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 823:0.0139\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 824:0.0138\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 825:0.0138\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 826:0.0138\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 827:0.0138\n",
      "train auc: 0.962138\n",
      "test auc: 0.897778\n",
      "loss at iter 828:0.0137\n",
      "train auc: 0.96288\n",
      "test auc: 0.897778\n",
      "loss at iter 829:0.0137\n",
      "train auc: 0.96288\n",
      "test auc: 0.897778\n",
      "loss at iter 830:0.0137\n",
      "train auc: 0.96288\n",
      "test auc: 0.895556\n",
      "loss at iter 831:0.0137\n",
      "train auc: 0.96288\n",
      "test auc: 0.895556\n",
      "loss at iter 832:0.0136\n",
      "train auc: 0.963623\n",
      "test auc: 0.895556\n",
      "loss at iter 833:0.0136\n",
      "train auc: 0.963623\n",
      "test auc: 0.897778\n",
      "loss at iter 834:0.0136\n",
      "train auc: 0.963623\n",
      "test auc: 0.897778\n",
      "loss at iter 835:0.0136\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 836:0.0135\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 837:0.0135\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 838:0.0135\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 839:0.0135\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 840:0.0135\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 841:0.0134\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 842:0.0134\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 843:0.0134\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 844:0.0134\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 845:0.0133\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 846:0.0133\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 847:0.0133\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 848:0.0133\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 849:0.0132\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 850:0.0132\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 851:0.0132\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 852:0.0132\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 853:0.0132\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 854:0.0131\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 855:0.0131\n",
      "train auc: 0.964365\n",
      "test auc: 0.897778\n",
      "loss at iter 856:0.0131\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 857:0.0131\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 858:0.0130\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 859:0.0130\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 860:0.0130\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 861:0.0130\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 862:0.0130\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 863:0.0129\n",
      "train auc: 0.965108\n",
      "test auc: 0.897778\n",
      "loss at iter 864:0.0129\n",
      "train auc: 0.965108\n",
      "test auc: 0.9\n",
      "loss at iter 865:0.0129\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 866:0.0129\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 867:0.0129\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 868:0.0128\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 869:0.0128\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 870:0.0128\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 871:0.0128\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 872:0.0127\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 873:0.0127\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 874:0.0127\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 875:0.0127\n",
      "train auc: 0.96585\n",
      "test auc: 0.9\n",
      "loss at iter 876:0.0127\n",
      "train auc: 0.966592\n",
      "test auc: 0.9\n",
      "loss at iter 877:0.0126\n",
      "train auc: 0.966592\n",
      "test auc: 0.9\n",
      "loss at iter 878:0.0126\n",
      "train auc: 0.966592\n",
      "test auc: 0.9\n",
      "loss at iter 879:0.0126\n",
      "train auc: 0.968077\n",
      "test auc: 0.9\n",
      "loss at iter 880:0.0126\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 881:0.0126\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 882:0.0125\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 883:0.0125\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 884:0.0125\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 885:0.0125\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 886:0.0125\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 887:0.0124\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 888:0.0124\n",
      "train auc: 0.96882\n",
      "test auc: 0.9\n",
      "loss at iter 889:0.0124\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 890:0.0124\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 891:0.0124\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 892:0.0123\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 893:0.0123\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 894:0.0123\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 895:0.0123\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 896:0.0123\n",
      "train auc: 0.969562\n",
      "test auc: 0.9\n",
      "loss at iter 897:0.0122\n",
      "train auc: 0.969562\n",
      "test auc: 0.902222\n",
      "loss at iter 898:0.0122\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 899:0.0122\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 900:0.0122\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 901:0.0122\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 902:0.0121\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 903:0.0121\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 904:0.0121\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 905:0.0121\n",
      "train auc: 0.970304\n",
      "test auc: 0.902222\n",
      "loss at iter 906:0.0121\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 907:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 908:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 909:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 910:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 911:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 912:0.0120\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 913:0.0119\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 914:0.0119\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 915:0.0119\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 916:0.0119\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 917:0.0119\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 918:0.0118\n",
      "train auc: 0.971047\n",
      "test auc: 0.902222\n",
      "loss at iter 919:0.0118\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 920:0.0118\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 921:0.0118\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 922:0.0118\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 923:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 924:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 925:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 926:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 927:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 928:0.0117\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 929:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 930:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.902222\n",
      "loss at iter 931:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 932:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 933:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 934:0.0116\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 935:0.0115\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 936:0.0115\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 937:0.0115\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 938:0.0115\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 939:0.0115\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 940:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 941:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 942:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 943:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 944:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 945:0.0114\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 946:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 947:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 948:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 949:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 950:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 951:0.0113\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 952:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 953:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 954:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 955:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 956:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 957:0.0112\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 958:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 959:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 960:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 961:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.904444\n",
      "loss at iter 962:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 963:0.0111\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 964:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 965:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 966:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 967:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 968:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 969:0.0110\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 970:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 971:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 972:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 973:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 974:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 975:0.0109\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 976:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 977:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.906667\n",
      "loss at iter 978:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 979:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 980:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 981:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 982:0.0108\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 983:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 984:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 985:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 986:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 987:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 988:0.0107\n",
      "train auc: 0.971789\n",
      "test auc: 0.908889\n",
      "loss at iter 989:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.908889\n",
      "loss at iter 990:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 991:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 992:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 993:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 994:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 995:0.0106\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 996:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 997:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 998:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 999:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1000:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1001:0.0105\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1002:0.0104\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1003:0.0104\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1004:0.0104\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1005:0.0104\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1006:0.0104\n",
      "train auc: 0.972532\n",
      "test auc: 0.911111\n",
      "loss at iter 1007:0.0104\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1008:0.0104\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1009:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1010:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1011:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1012:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1013:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1014:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1015:0.0103\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1016:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1017:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1018:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1019:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1020:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1021:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1022:0.0102\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1023:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1024:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1025:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1026:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1027:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1028:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1029:0.0101\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1030:0.0100\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1031:0.0100\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1032:0.0100\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1033:0.0100\n",
      "train auc: 0.973274\n",
      "test auc: 0.911111\n",
      "loss at iter 1034:0.0100\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1035:0.0100\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1036:0.0100\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1037:0.0099\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1038:0.0099\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1039:0.0099\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1040:0.0099\n",
      "train auc: 0.974016\n",
      "test auc: 0.913333\n",
      "loss at iter 1041:0.0099\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1042:0.0099\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1043:0.0099\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1044:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1045:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1046:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1047:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1048:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1049:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1050:0.0098\n",
      "train auc: 0.974759\n",
      "test auc: 0.913333\n",
      "loss at iter 1051:0.0098\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1052:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1053:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1054:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1055:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1056:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1057:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1058:0.0097\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1059:0.0096\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1060:0.0096\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1061:0.0096\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1062:0.0096\n",
      "train auc: 0.975501\n",
      "test auc: 0.913333\n",
      "loss at iter 1063:0.0096\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1064:0.0096\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1065:0.0096\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1066:0.0096\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1067:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1068:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1069:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1070:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1071:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1072:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1073:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1074:0.0095\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1075:0.0094\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1076:0.0094\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1077:0.0094\n",
      "train auc: 0.976243\n",
      "test auc: 0.913333\n",
      "loss at iter 1078:0.0094\n",
      "train auc: 0.976986\n",
      "test auc: 0.913333\n",
      "loss at iter 1079:0.0094\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1080:0.0094\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1081:0.0094\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1082:0.0094\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1083:0.0093\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1084:0.0093\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1085:0.0093\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1086:0.0093\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1087:0.0093\n",
      "train auc: 0.977728\n",
      "test auc: 0.913333\n",
      "loss at iter 1088:0.0093\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1089:0.0093\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1090:0.0093\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1091:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1092:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1093:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1094:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1095:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1096:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1097:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1098:0.0092\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1099:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1100:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1101:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1102:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1103:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1104:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1105:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1106:0.0091\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1107:0.0090\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1108:0.0090\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1109:0.0090\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1110:0.0090\n",
      "train auc: 0.978471\n",
      "test auc: 0.913333\n",
      "loss at iter 1111:0.0090\n",
      "train auc: 0.979213\n",
      "test auc: 0.913333\n",
      "loss at iter 1112:0.0090\n",
      "train auc: 0.979213\n",
      "test auc: 0.913333\n",
      "loss at iter 1113:0.0090\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1114:0.0090\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1115:0.0090\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1116:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1117:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1118:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1119:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1120:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1121:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1122:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1123:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1124:0.0089\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1125:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1126:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1127:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1128:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1129:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1130:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1131:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1132:0.0088\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1133:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1134:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1135:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1136:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1137:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1138:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1139:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.915556\n",
      "loss at iter 1140:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1141:0.0087\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1142:0.0086\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1143:0.0086\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1144:0.0086\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1145:0.0086\n",
      "train auc: 0.979213\n",
      "test auc: 0.917778\n",
      "loss at iter 1146:0.0086\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1147:0.0086\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1148:0.0086\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1149:0.0086\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1150:0.0086\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1151:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1152:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1153:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1154:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1155:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1156:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1157:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1158:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1159:0.0085\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1160:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1161:0.0084\n",
      "train auc: 0.979955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.917778\n",
      "loss at iter 1162:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1163:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1164:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1165:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1166:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1167:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1168:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1169:0.0084\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1170:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1171:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1172:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1173:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1174:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1175:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1176:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1177:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1178:0.0083\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1179:0.0082\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1180:0.0082\n",
      "train auc: 0.979955\n",
      "test auc: 0.917778\n",
      "loss at iter 1181:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1182:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1183:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1184:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1185:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1186:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1187:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1188:0.0082\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1189:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1190:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1191:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1192:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1193:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1194:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1195:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1196:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1197:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1198:0.0081\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1199:0.0080\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1200:0.0080\n",
      "train auc: 0.980698\n",
      "test auc: 0.917778\n",
      "loss at iter 1201:0.0080\n",
      "train auc: 0.980698\n",
      "test auc: 0.92\n",
      "loss at iter 1202:0.0080\n",
      "train auc: 0.980698\n",
      "test auc: 0.92\n",
      "loss at iter 1203:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1204:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1205:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1206:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1207:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1208:0.0080\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1209:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1210:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1211:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1212:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1213:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1214:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1215:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1216:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1217:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1218:0.0079\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1219:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1220:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1221:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1222:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1223:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1224:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1225:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1226:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1227:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1228:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1229:0.0078\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1230:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1231:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1232:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1233:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1234:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1235:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1236:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1237:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1238:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1239:0.0077\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1240:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1241:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1242:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1243:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1244:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1245:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1246:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1247:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1248:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1249:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1250:0.0076\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1251:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1252:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1253:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1254:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1255:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1256:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1257:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1258:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1259:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1260:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1261:0.0075\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1262:0.0074\n",
      "train auc: 0.98144\n",
      "test auc: 0.92\n",
      "loss at iter 1263:0.0074\n",
      "train auc: 0.982183\n",
      "test auc: 0.92\n",
      "loss at iter 1264:0.0074\n",
      "train auc: 0.982183\n",
      "test auc: 0.92\n",
      "loss at iter 1265:0.0074\n",
      "train auc: 0.982183\n",
      "test auc: 0.92\n",
      "loss at iter 1266:0.0074\n",
      "train auc: 0.982183\n",
      "test auc: 0.92\n",
      "loss at iter 1267:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1268:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1269:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1270:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1271:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1272:0.0074\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1273:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1274:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1275:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1276:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1277:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1278:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1279:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1280:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1281:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1282:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1283:0.0073\n",
      "train auc: 0.982925\n",
      "test auc: 0.92\n",
      "loss at iter 1284:0.0072\n",
      "train auc: 0.982925\n",
      "test auc: 0.922222\n",
      "loss at iter 1285:0.0072\n",
      "train auc: 0.982925\n",
      "test auc: 0.922222\n",
      "loss at iter 1286:0.0072\n",
      "train auc: 0.982925\n",
      "test auc: 0.922222\n",
      "loss at iter 1287:0.0072\n",
      "train auc: 0.982925\n",
      "test auc: 0.922222\n",
      "loss at iter 1288:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1289:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1290:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1291:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1292:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1293:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1294:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1295:0.0072\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1296:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1297:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1298:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1299:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1300:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1301:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1302:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1303:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1304:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1305:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1306:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1307:0.0071\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1308:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1309:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1310:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1311:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1312:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1313:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1314:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.922222\n",
      "loss at iter 1315:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1316:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1317:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1318:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1319:0.0070\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1320:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1321:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1322:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1323:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1324:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1325:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1326:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1327:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1328:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1329:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1330:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1331:0.0069\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1332:0.0068\n",
      "train auc: 0.983667\n",
      "test auc: 0.924444\n",
      "loss at iter 1333:0.0068\n",
      "train auc: 0.983667\n",
      "test auc: 0.926667\n",
      "loss at iter 1334:0.0068\n",
      "train auc: 0.983667\n",
      "test auc: 0.926667\n",
      "loss at iter 1335:0.0068\n",
      "train auc: 0.983667\n",
      "test auc: 0.926667\n",
      "loss at iter 1336:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1337:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1338:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1339:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1340:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1341:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1342:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1343:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1344:0.0068\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1345:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1346:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1347:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1348:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1349:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1350:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1351:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1352:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1353:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1354:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1355:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1356:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1357:0.0067\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1358:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1359:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1360:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1361:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1362:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1363:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1364:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1365:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1366:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1367:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1368:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1369:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1370:0.0066\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1371:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1372:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1373:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1374:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1375:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1376:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1377:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1378:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1379:0.0065\n",
      "train auc: 0.98441\n",
      "test auc: 0.926667\n",
      "loss at iter 1380:0.0065\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1381:0.0065\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1382:0.0065\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1383:0.0065\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1384:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1385:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1386:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1387:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1388:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1389:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1390:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1391:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1392:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1393:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1394:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1395:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1396:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.926667\n",
      "loss at iter 1397:0.0064\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1398:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1399:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1400:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1401:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1402:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1403:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1404:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1405:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1406:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1407:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1408:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1409:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1410:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1411:0.0063\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1412:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1413:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1414:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1415:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1416:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1417:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1418:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1419:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1420:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1421:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1422:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1423:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1424:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1425:0.0062\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1426:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1427:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1428:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1429:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1430:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1431:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1432:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1433:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1434:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1435:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1436:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1437:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1438:0.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1439:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1440:0.0061\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1441:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1442:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1443:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1444:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1445:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1446:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1447:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1448:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1449:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1450:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1451:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1452:0.0060\n",
      "train auc: 0.985152\n",
      "test auc: 0.928889\n",
      "loss at iter 1453:0.0060\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1454:0.0060\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1455:0.0060\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1456:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1457:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1458:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1459:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1460:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1461:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1462:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1463:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1464:0.0059\n",
      "train auc: 0.985895\n",
      "test auc: 0.928889\n",
      "loss at iter 1465:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1466:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1467:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1468:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1469:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1470:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1471:0.0059\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1472:0.0058\n",
      "train auc: 0.986637\n",
      "test auc: 0.928889\n",
      "loss at iter 1473:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1474:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1475:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1476:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1477:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1478:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1479:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1480:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1481:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1482:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1483:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1484:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1485:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1486:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1487:0.0058\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1488:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1489:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1490:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1491:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1492:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1493:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1494:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1495:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1496:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1497:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1498:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1499:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1500:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1501:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1502:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1503:0.0057\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1504:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1505:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1506:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1507:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1508:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1509:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1510:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1511:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1512:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1513:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1514:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1515:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1516:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1517:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1518:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1519:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1520:0.0056\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1521:0.0055\n",
      "train auc: 0.987379\n",
      "test auc: 0.928889\n",
      "loss at iter 1522:0.0055\n",
      "train auc: 0.988122\n",
      "test auc: 0.928889\n",
      "loss at iter 1523:0.0055\n",
      "train auc: 0.988122\n",
      "test auc: 0.928889\n",
      "loss at iter 1524:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1525:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1526:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1527:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1528:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1529:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1530:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1531:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1532:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.928889\n",
      "loss at iter 1533:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.931111\n",
      "loss at iter 1534:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.931111\n",
      "loss at iter 1535:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.931111\n",
      "loss at iter 1536:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.931111\n",
      "loss at iter 1537:0.0055\n",
      "train auc: 0.988864\n",
      "test auc: 0.931111\n",
      "loss at iter 1538:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1539:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1540:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1541:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1542:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1543:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1544:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1545:0.0054\n",
      "train auc: 0.988864\n",
      "test auc: 0.933333\n",
      "loss at iter 1546:0.0054\n",
      "train auc: 0.989607\n",
      "test auc: 0.933333\n",
      "loss at iter 1547:0.0054\n",
      "train auc: 0.989607\n",
      "test auc: 0.933333\n",
      "loss at iter 1548:0.0054\n",
      "train auc: 0.989607\n",
      "test auc: 0.933333\n",
      "loss at iter 1549:0.0054\n",
      "train auc: 0.989607\n",
      "test auc: 0.933333\n",
      "loss at iter 1550:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1551:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1552:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1553:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1554:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1555:0.0054\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1556:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1557:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1558:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1559:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1560:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1561:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1562:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1563:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.933333\n",
      "loss at iter 1564:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1565:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1566:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1567:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1568:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1569:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1570:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1571:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1572:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1573:0.0053\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1574:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1575:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1576:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1577:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1578:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1579:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1580:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1581:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1582:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1583:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1584:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1585:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1586:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1587:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1588:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1589:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1590:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1591:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1592:0.0052\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1593:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.935556\n",
      "loss at iter 1594:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1595:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1596:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1597:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1598:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1599:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1600:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1601:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1602:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1603:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1604:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1605:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1606:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1607:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1608:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1609:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1610:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1611:0.0051\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1612:0.0050\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1613:0.0050\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1614:0.0050\n",
      "train auc: 0.990349\n",
      "test auc: 0.937778\n",
      "loss at iter 1615:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1616:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1617:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1618:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1619:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1620:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1621:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1622:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1623:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1624:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1625:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1626:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1627:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1628:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1629:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1630:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1631:0.0050\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1632:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1633:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1634:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1635:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1636:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1637:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1638:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1639:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1640:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1641:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1642:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1643:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1644:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1645:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1646:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1647:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1648:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1649:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1650:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1651:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1652:0.0049\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1653:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1654:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1655:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1656:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1657:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1658:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1659:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1660:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1661:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1662:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1663:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1664:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1665:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1666:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1667:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1668:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1669:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1670:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1671:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1672:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1673:0.0048\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1674:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1675:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1676:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1677:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1678:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1679:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1680:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1681:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.937778\n",
      "loss at iter 1682:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.94\n",
      "loss at iter 1683:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.94\n",
      "loss at iter 1684:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.94\n",
      "loss at iter 1685:0.0047\n",
      "train auc: 0.991091\n",
      "test auc: 0.94\n",
      "loss at iter 1686:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1687:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1688:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1689:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1690:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1691:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1692:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1693:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1694:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1695:0.0047\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1696:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1697:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1698:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1699:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1700:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1701:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1702:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1703:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1704:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1705:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1706:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1707:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1708:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1709:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1710:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1711:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1712:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1713:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1714:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1715:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1716:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1717:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1718:0.0046\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1719:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1720:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1721:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1722:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1723:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1724:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1725:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1726:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1727:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1728:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1729:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1730:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1731:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1732:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1733:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1734:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1735:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1736:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1737:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1738:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1739:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1740:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1741:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1742:0.0045\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1743:0.0044\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1744:0.0044\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1745:0.0044\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1746:0.0044\n",
      "train auc: 0.991834\n",
      "test auc: 0.94\n",
      "loss at iter 1747:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1748:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1749:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1750:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1751:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1752:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1753:0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1754:0.0044\n",
      "train auc: 0.992576\n",
      "test auc: 0.94\n",
      "loss at iter 1755:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1756:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1757:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1758:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1759:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1760:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1761:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1762:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1763:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1764:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1765:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1766:0.0044\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1767:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1768:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1769:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1770:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1771:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1772:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1773:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1774:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1775:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1776:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1777:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1778:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1779:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1780:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1781:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1782:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1783:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1784:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1785:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1786:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1787:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1788:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1789:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1790:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1791:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1792:0.0043\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1793:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1794:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1795:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1796:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1797:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1798:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1799:0.0042\n",
      "train auc: 0.993318\n",
      "test auc: 0.94\n",
      "loss at iter 1800:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1801:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1802:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1803:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1804:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1805:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1806:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1807:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1808:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1809:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1810:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1811:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1812:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1813:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1814:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1815:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1816:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1817:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1818:0.0042\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1819:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1820:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1821:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1822:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1823:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1824:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1825:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1826:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1827:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1828:0.0041\n",
      "train auc: 0.994061\n",
      "test auc: 0.94\n",
      "loss at iter 1829:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1830:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1831:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1832:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1833:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1834:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1835:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1836:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1837:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1838:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1839:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1840:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1841:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1842:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1843:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1844:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1845:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1846:0.0041\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1847:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1848:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1849:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1850:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1851:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1852:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1853:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1854:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1855:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1856:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1857:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1858:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1859:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1860:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1861:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1862:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1863:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1864:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1865:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1866:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1867:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1868:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1869:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1870:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1871:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1872:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1873:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.937778\n",
      "loss at iter 1874:0.0040\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1875:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1876:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1877:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1878:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1879:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1880:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1881:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1882:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1883:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1884:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1885:0.0039\n",
      "train auc: 0.994803\n",
      "test auc: 0.94\n",
      "loss at iter 1886:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1887:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1888:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1889:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1890:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1891:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1892:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1893:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1894:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1895:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1896:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1897:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1898:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1899:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1900:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1901:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1902:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1903:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1904:0.0039\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1905:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1906:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1907:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1908:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1909:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1910:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1911:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1912:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1913:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1914:0.0038\n",
      "train auc: 0.995546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.94\n",
      "loss at iter 1915:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1916:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1917:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1918:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.94\n",
      "loss at iter 1919:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1920:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1921:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1922:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1923:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1924:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1925:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1926:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1927:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1928:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1929:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1930:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1931:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1932:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1933:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1934:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1935:0.0038\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1936:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1937:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1938:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1939:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1940:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1941:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1942:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1943:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1944:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1945:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1946:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1947:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1948:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1949:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1950:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1951:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1952:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1953:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1954:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1955:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1956:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1957:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1958:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1959:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1960:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1961:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1962:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1963:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1964:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1965:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1966:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1967:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1968:0.0037\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1969:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1970:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1971:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1972:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1973:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1974:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1975:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1976:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1977:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1978:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1979:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1980:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1981:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1982:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1983:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1984:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1985:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1986:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1987:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1988:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1989:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1990:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1991:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1992:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1993:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1994:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1995:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1996:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1997:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1998:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 1999:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2000:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2001:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2002:0.0036\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2003:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2004:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2005:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2006:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2007:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2008:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2009:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2010:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2011:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2012:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2013:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2014:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2015:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2016:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2017:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2018:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2019:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2020:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2021:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2022:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2023:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2024:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2025:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2026:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2027:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2028:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2029:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2030:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2031:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2032:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2033:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2034:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2035:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2036:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2037:0.0035\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2038:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.942222\n",
      "loss at iter 2039:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2040:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2041:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2042:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2043:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2044:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2045:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2046:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2047:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2048:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2049:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2050:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2051:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2052:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2053:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2054:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2055:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2056:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2057:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2058:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2059:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2060:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2061:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2062:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2063:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2064:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2065:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2066:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2067:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2068:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2069:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2070:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 2071:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2072:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2073:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2074:0.0034\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2075:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2076:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2077:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2078:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2079:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2080:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2081:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2082:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2083:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2084:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2085:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2086:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2087:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2088:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2089:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2090:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2091:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2092:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2093:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2094:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2095:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2096:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2097:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2098:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2099:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2100:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2101:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2102:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2103:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2104:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2105:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2106:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2107:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2108:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2109:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2110:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2111:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2112:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2113:0.0033\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2114:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2115:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2116:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2117:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2118:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2119:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2120:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2121:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2122:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2123:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2124:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2125:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2126:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2127:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2128:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2129:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2130:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2131:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2132:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2133:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2134:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2135:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2136:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2137:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2138:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2139:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2140:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2141:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2142:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2143:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2144:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2145:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2146:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2147:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2148:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2149:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2150:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2151:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2152:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2153:0.0032\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2154:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2155:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2156:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2157:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2158:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2159:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2160:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2161:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2162:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2163:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2164:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2165:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2166:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2167:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2168:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2169:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2170:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2171:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2172:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2173:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2174:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2175:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2176:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2177:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2178:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2179:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2180:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2181:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2182:0.0031\n",
      "train auc: 0.995546\n",
      "test auc: 0.944444\n",
      "loss at iter 2183:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2184:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2185:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2186:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2187:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2188:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2189:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2190:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2191:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2192:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2193:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2194:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2195:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2196:0.0031\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2197:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2198:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2199:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2200:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2201:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2202:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2203:0.0030\n",
      "train auc: 0.996288\n",
      "test auc: 0.946667\n",
      "loss at iter 2204:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2205:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2206:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2207:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2208:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2209:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2210:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2211:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2212:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2213:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2214:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2215:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2216:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2217:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2218:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2219:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2220:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2221:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2222:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2223:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2224:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2225:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2226:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2227:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2228:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 2229:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2230:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2231:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2232:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2233:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2234:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2235:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2236:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2237:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2238:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2239:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2240:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2241:0.0030\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2242:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2243:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2244:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2245:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2246:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2247:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.946667\n",
      "loss at iter 2248:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2249:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2250:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2251:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2252:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2253:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2254:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2255:0.0029\n",
      "train auc: 0.99703\n",
      "test auc: 0.948889\n",
      "loss at iter 2256:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2257:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2258:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2259:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2260:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2261:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2262:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2263:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2264:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2265:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2266:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2267:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2268:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2269:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2270:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2271:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2272:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2273:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2274:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2275:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2276:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2277:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2278:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2279:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2280:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2281:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2282:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2283:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2284:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2285:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2286:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2287:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2288:0.0029\n",
      "train auc: 0.997773\n",
      "test auc: 0.948889\n",
      "loss at iter 2289:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2290:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2291:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2292:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2293:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2294:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2295:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2296:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2297:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2298:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2299:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2300:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2301:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2302:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2303:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2304:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2305:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2306:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2307:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2308:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2309:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2310:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2311:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2312:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2313:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2314:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2315:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2316:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2317:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2318:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2319:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2320:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2321:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2322:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2323:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2324:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2325:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2326:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.948889\n",
      "loss at iter 2327:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2328:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2329:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2330:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2331:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2332:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2333:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2334:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2335:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2336:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2337:0.0028\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2338:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2339:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2340:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2341:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2342:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2343:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2344:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2345:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2346:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2347:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2348:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2349:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2350:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2351:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2352:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2353:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2354:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2355:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2356:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2357:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2358:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2359:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2360:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2361:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2362:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2363:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2364:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2365:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2366:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2367:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2368:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2369:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2370:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2371:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2372:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2373:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2374:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2375:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2376:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2377:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2378:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2379:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2380:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2381:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2382:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 2383:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2384:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2385:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2386:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2387:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2388:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2389:0.0027\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2390:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2391:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2392:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2393:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2394:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2395:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2396:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2397:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2398:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2399:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2400:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2401:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2402:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2403:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2404:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2405:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2406:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2407:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2408:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2409:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2410:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2411:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2412:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2413:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2414:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2415:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2416:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2417:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2418:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2419:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2420:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2421:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2422:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2423:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2424:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2425:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2426:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2427:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2428:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2429:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2430:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2431:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2432:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2433:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2434:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2435:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2436:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2437:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2438:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2439:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2440:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2441:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2442:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2443:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2444:0.0026\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2445:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2446:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2447:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2448:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2449:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2450:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2451:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2452:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2453:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2454:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2455:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2456:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2457:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2458:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2459:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2460:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2461:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2462:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2463:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2464:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2465:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2466:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2467:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2468:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2469:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2470:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2471:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2472:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2473:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2474:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2475:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2476:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2477:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2478:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2479:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2480:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2481:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2482:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2483:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2484:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2485:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2486:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2487:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2488:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2489:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2490:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2491:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2492:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2493:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2494:0.0025\n",
      "train auc: 0.998515\n",
      "test auc: 0.951111\n",
      "loss at iter 2495:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2496:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2497:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2498:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2499:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2500:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2501:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2502:0.0025\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2503:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2504:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2505:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2506:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2507:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2508:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2509:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2510:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2511:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2512:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2513:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2514:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2515:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2516:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2517:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2518:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2519:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2520:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2521:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2522:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2523:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2524:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2525:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2526:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2527:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2528:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2529:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2530:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2531:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2532:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2533:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2534:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2535:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2536:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2537:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2538:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2539:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2540:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2541:0.0024\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.951111\n",
      "loss at iter 2542:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2543:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2544:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2545:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2546:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2547:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2548:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2549:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2550:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2551:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2552:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2553:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2554:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2555:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2556:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2557:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2558:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2559:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2560:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2561:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2562:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2563:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2564:0.0024\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2565:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2566:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2567:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2568:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2569:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2570:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2571:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2572:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2573:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2574:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2575:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2576:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2577:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2578:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2579:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2580:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2581:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2582:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2583:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2584:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2585:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2586:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2587:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2588:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2589:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2590:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2591:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2592:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2593:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2594:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2595:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2596:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2597:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2598:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2599:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2600:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2601:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2602:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2603:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2604:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2605:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2606:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2607:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2608:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2609:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2610:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2611:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2612:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2613:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2614:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2615:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2616:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2617:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2618:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2619:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2620:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2621:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2622:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2623:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2624:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2625:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2626:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2627:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2628:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2629:0.0023\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2630:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2631:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2632:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2633:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2634:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2635:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2636:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2637:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2638:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2639:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2640:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2641:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2642:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2643:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2644:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2645:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2646:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2647:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2648:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2649:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2650:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2651:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2652:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2653:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2654:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2655:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2656:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2657:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2658:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2659:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2660:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2661:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2662:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2663:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2664:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2665:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2666:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2667:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2668:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2669:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2670:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2671:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2672:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2673:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2674:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2675:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2676:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2677:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2678:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2679:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2680:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2681:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2682:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2683:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2684:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2685:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2686:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2687:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2688:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2689:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2690:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2691:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2692:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2693:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2694:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2695:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2696:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2697:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2698:0.0022\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2699:0.0021\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.951111\n",
      "loss at iter 2700:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2701:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2702:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2703:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2704:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2705:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2706:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2707:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2708:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2709:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2710:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2711:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2712:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2713:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2714:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2715:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2716:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2717:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2718:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2719:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2720:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2721:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2722:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2723:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2724:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2725:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2726:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2727:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2728:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2729:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2730:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2731:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2732:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2733:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2734:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2735:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2736:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2737:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2738:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2739:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2740:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2741:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2742:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2743:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2744:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2745:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2746:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2747:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2748:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2749:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2750:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2751:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2752:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2753:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2754:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2755:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2756:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2757:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2758:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2759:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2760:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2761:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2762:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2763:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2764:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2765:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2766:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2767:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2768:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2769:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2770:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2771:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2772:0.0021\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2773:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2774:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2775:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2776:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2777:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2778:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2779:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2780:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2781:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2782:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2783:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2784:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2785:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2786:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2787:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2788:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2789:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2790:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2791:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2792:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2793:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2794:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2795:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2796:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2797:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2798:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2799:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2800:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2801:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2802:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2803:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2804:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2805:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2806:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2807:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2808:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2809:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2810:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2811:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2812:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2813:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2814:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2815:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2816:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2817:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2818:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2819:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2820:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2821:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2822:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2823:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2824:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2825:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2826:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2827:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2828:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2829:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2830:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2831:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2832:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2833:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2834:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2835:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2836:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2837:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2838:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2839:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2840:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2841:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2842:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2843:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2844:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2845:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2846:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2847:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2848:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2849:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2850:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2851:0.0020\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2852:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2853:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2854:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2855:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2856:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2857:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2858:0.0019\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.951111\n",
      "loss at iter 2859:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2860:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2861:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2862:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2863:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2864:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2865:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2866:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2867:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2868:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2869:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2870:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 2871:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2872:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2873:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2874:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2875:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2876:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2877:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2878:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2879:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2880:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2881:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2882:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2883:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2884:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2885:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2886:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2887:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2888:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2889:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2890:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2891:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2892:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2893:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2894:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2895:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2896:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2897:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2898:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2899:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2900:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2901:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2902:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2903:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2904:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2905:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2906:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2907:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2908:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2909:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2910:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2911:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2912:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2913:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2914:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2915:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2916:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2917:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2918:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2919:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2920:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2921:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2922:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2923:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2924:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2925:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2926:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2927:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2928:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2929:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2930:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2931:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2932:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2933:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2934:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2935:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2936:0.0019\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2937:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2938:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2939:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2940:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2941:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2942:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2943:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2944:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2945:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2946:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2947:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2948:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2949:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2950:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2951:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2952:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2953:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2954:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2955:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2956:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2957:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2958:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2959:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2960:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2961:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2962:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2963:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2964:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2965:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2966:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2967:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2968:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2969:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2970:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2971:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2972:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2973:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2974:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2975:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2976:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2977:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2978:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2979:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2980:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2981:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2982:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2983:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2984:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2985:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2986:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2987:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2988:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2989:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2990:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2991:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2992:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2993:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2994:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2995:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2996:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2997:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2998:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 2999:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3000:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3001:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3002:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3003:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3004:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3005:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3006:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3007:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3008:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3009:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3010:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3011:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3012:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3013:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3014:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3015:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3016:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3017:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3018:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3019:0.0018\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.953333\n",
      "loss at iter 3020:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3021:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3022:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3023:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3024:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3025:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3026:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3027:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3028:0.0018\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3029:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3030:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3031:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3032:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3033:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3034:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3035:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3036:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3037:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3038:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3039:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3040:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3041:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3042:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3043:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3044:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3045:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3046:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3047:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3048:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3049:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3050:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3051:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3052:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3053:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3054:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3055:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3056:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3057:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3058:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3059:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3060:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3061:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3062:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3063:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3064:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3065:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3066:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3067:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3068:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3069:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3070:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3071:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3072:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3073:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3074:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3075:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3076:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3077:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3078:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3079:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3080:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3081:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3082:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3083:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3084:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3085:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3086:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3087:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3088:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3089:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3090:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3091:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3092:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3093:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3094:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3095:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3096:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3097:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3098:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3099:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3100:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3101:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3102:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3103:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3104:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3105:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3106:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3107:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3108:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3109:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3110:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3111:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3112:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3113:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3114:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3115:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3116:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3117:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3118:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3119:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3120:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3121:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3122:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3123:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3124:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3125:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3126:0.0017\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3127:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3128:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3129:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3130:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3131:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3132:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3133:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3134:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3135:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3136:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3137:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3138:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3139:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3140:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3141:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3142:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3143:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3144:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3145:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3146:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3147:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3148:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3149:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3150:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3151:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3152:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3153:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3154:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3155:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3156:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3157:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3158:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3159:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3160:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3161:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3162:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3163:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3164:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3165:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3166:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3167:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3168:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3169:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3170:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3171:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3172:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3173:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3174:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3175:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3176:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3177:0.0016\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.953333\n",
      "loss at iter 3178:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3179:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3180:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3181:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3182:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3183:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3184:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3185:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3186:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3187:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3188:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3189:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3190:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3191:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3192:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3193:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3194:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3195:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3196:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3197:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3198:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3199:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3200:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3201:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3202:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3203:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3204:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3205:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3206:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3207:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3208:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3209:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3210:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3211:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3212:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3213:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3214:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3215:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3216:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3217:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3218:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3219:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3220:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3221:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3222:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3223:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3224:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3225:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3226:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3227:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3228:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3229:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3230:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3231:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3232:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3233:0.0016\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3234:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3235:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3236:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3237:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3238:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3239:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3240:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3241:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3242:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3243:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3244:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3245:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3246:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.953333\n",
      "loss at iter 3247:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3248:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3249:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3250:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3251:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3252:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3253:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3254:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3255:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3256:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3257:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3258:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3259:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3260:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3261:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3262:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3263:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3264:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3265:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3266:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3267:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3268:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3269:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3270:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3271:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3272:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3273:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3274:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3275:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3276:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3277:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3278:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3279:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3280:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3281:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3282:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3283:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3284:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3285:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3286:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3287:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3288:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3289:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3290:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3291:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3292:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3293:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3294:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3295:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3296:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3297:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3298:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3299:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3300:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3301:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3302:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3303:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3304:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3305:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3306:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3307:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3308:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3309:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3310:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3311:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3312:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3313:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3314:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3315:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3316:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3317:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3318:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3319:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3320:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3321:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3322:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3323:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3324:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3325:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3326:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3327:0.0015\n",
      "train auc: 0.999258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.951111\n",
      "loss at iter 3328:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3329:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3330:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3331:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3332:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3333:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3334:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3335:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3336:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3337:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3338:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3339:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3340:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3341:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3342:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3343:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3344:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3345:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3346:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3347:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3348:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3349:0.0015\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3350:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3351:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3352:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3353:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3354:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3355:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3356:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3357:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3358:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3359:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3360:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3361:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3362:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3363:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3364:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3365:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3366:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3367:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3368:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3369:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3370:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3371:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3372:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3373:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3374:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3375:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3376:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3377:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3378:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3379:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3380:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3381:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3382:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3383:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3384:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3385:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3386:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3387:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3388:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3389:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3390:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3391:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3392:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3393:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3394:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3395:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3396:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3397:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3398:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3399:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3400:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3401:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3402:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3403:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3404:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3405:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3406:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3407:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3408:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3409:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3410:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3411:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3412:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3413:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3414:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3415:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3416:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3417:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3418:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3419:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3420:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3421:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3422:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3423:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3424:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3425:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3426:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3427:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3428:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3429:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3430:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3431:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3432:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3433:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3434:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3435:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3436:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3437:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3438:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3439:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3440:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3441:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3442:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3443:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3444:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3445:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3446:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3447:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3448:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3449:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3450:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3451:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3452:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3453:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3454:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3455:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3456:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3457:0.0014\n",
      "train auc: 0.999258\n",
      "test auc: 0.951111\n",
      "loss at iter 3458:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3459:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3460:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3461:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3462:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3463:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3464:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3465:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3466:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3467:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3468:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3469:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3470:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3471:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3472:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3473:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3474:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3475:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3476:0.0014\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3477:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3478:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3479:0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3480:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3481:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3482:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3483:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3484:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3485:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3486:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3487:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3488:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3489:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3490:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3491:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3492:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3493:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3494:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3495:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3496:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3497:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3498:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3499:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3500:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3501:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3502:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3503:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3504:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3505:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3506:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3507:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3508:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3509:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3510:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3511:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3512:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3513:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3514:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3515:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3516:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3517:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3518:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3519:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3520:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3521:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3522:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3523:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3524:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3525:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3526:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3527:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3528:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3529:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3530:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3531:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3532:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3533:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3534:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3535:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3536:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3537:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3538:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3539:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3540:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3541:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3542:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3543:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3544:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3545:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3546:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3547:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3548:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3549:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3550:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.951111\n",
      "loss at iter 3551:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3552:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3553:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3554:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3555:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3556:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3557:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3558:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3559:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3560:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3561:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3562:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3563:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3564:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3565:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3566:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3567:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3568:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3569:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3570:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3571:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3572:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3573:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3574:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3575:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3576:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3577:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3578:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3579:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3580:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3581:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3582:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3583:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3584:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3585:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3586:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3587:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3588:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3589:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3590:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3591:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3592:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3593:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3594:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3595:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3596:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3597:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3598:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3599:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3600:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3601:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3602:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3603:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3604:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3605:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3606:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3607:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3608:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3609:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3610:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3611:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3612:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3613:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3614:0.0013\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3615:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3616:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3617:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3618:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3619:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3620:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3621:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3622:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3623:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3624:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3625:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3626:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3627:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3628:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3629:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3630:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3631:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3632:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3633:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3634:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3635:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3636:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 3637:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3638:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3639:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3640:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3641:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3642:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3643:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3644:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3645:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3646:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3647:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3648:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3649:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3650:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3651:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3652:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3653:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3654:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3655:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3656:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3657:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3658:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3659:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3660:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3661:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3662:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3663:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3664:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3665:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3666:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3667:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3668:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3669:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3670:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3671:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3672:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3673:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3674:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3675:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3676:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3677:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3678:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3679:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3680:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3681:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3682:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3683:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3684:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3685:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3686:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3687:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3688:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3689:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3690:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3691:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3692:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3693:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3694:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3695:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3696:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3697:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3698:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3699:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3700:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3701:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3702:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3703:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3704:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3705:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3706:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3707:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3708:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3709:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3710:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3711:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3712:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3713:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3714:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3715:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3716:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3717:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3718:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3719:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3720:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3721:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3722:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3723:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3724:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3725:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3726:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3727:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3728:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3729:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3730:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3731:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3732:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3733:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3734:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3735:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3736:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3737:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3738:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3739:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3740:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3741:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3742:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3743:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3744:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3745:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3746:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3747:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3748:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3749:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3750:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3751:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3752:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3753:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3754:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3755:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3756:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3757:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3758:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3759:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3760:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3761:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3762:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3763:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3764:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3765:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3766:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3767:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3768:0.0012\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3769:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3770:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3771:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3772:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3773:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3774:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3775:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3776:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3777:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3778:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3779:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3780:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3781:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3782:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3783:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3784:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3785:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3786:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3787:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3788:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3789:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3790:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3791:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3792:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3793:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3794:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3795:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3796:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3797:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3798:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3799:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3800:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3801:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3802:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3803:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3804:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3805:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3806:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3807:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3808:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3809:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3810:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3811:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3812:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3813:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3814:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3815:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3816:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3817:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3818:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3819:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3820:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3821:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3822:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3823:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3824:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3825:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3826:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3827:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3828:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3829:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3830:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3831:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3832:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3833:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3834:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3835:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3836:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3837:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3838:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3839:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3840:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3841:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3842:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3843:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3844:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3845:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3846:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3847:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3848:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3849:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 3850:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3851:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3852:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3853:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3854:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3855:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3856:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3857:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3858:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3859:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3860:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3861:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3862:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3863:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3864:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3865:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3866:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3867:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3868:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3869:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3870:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3871:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3872:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3873:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3874:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3875:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3876:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3877:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3878:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3879:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3880:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3881:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3882:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3883:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3884:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3885:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3886:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3887:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3888:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3889:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3890:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3891:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3892:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3893:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3894:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3895:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3896:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3897:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3898:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3899:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3900:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3901:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3902:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3903:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3904:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3905:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3906:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3907:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3908:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3909:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3910:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3911:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3912:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3913:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3914:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3915:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3916:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3917:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3918:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3919:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3920:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3921:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3922:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3923:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3924:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3925:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3926:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3927:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3928:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3929:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3930:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3931:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3932:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3933:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3934:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3935:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3936:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3937:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3938:0.0011\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3939:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3940:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3941:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3942:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3943:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3944:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3945:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3946:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3947:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3948:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3949:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3950:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3951:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3952:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3953:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3954:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3955:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3956:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3957:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3958:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3959:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3960:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3961:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3962:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3963:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3964:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3965:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3966:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3967:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3968:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3969:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3970:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3971:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3972:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3973:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3974:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3975:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3976:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3977:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3978:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3979:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3980:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3981:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3982:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3983:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3984:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3985:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3986:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3987:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3988:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3989:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3990:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3991:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3992:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3993:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3994:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3995:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3996:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3997:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3998:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "loss at iter 3999:0.0010\n",
      "train auc: 1.0\n",
      "test auc: 0.953333\n",
      "resulting weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f5ce52adf28>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAD8CAYAAADwpviIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAiwiGisBFhGNlQCLiMb6P4IjQNWy2lVqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce5544f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(4000):\n",
    "    sess.run(optimizer, {input_X: X_train, input_y: y_train})\n",
    "    loss_i = sess.run(loss, {input_X: X_train, input_y: y_train})\n",
    "\n",
    "    print (\"loss at iter %i:%.4f\"%(i, loss_i))\n",
    "\n",
    "    print (\"train auc:\", sess.run(accuracy, feed_dict={input_X: X_train, input_y: y_train}))\n",
    "    print (\"test auc:\", sess.run(accuracy, feed_dict={input_X: X_test, input_y: y_test}))\n",
    "\n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(sess.run(weights)[:, 0].reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my1stNN\n",
    "__[basic part 4 points max]__\n",
    "Your ultimate task for this week is to build your first neural network [almost] from scratch and pure tensorflow.\n",
    "\n",
    "This time you will same digit recognition problem, but at a larger scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples\n",
    "\n",
    "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) NN should already have ive you an edge over logistic regression.\n",
    "\n",
    "__[bonus score]__\n",
    "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! The milestones would be 95%/97.5%/98.5% accuraсy on test set.\n",
    "\n",
    "__SPOILER!__\n",
    "At the end of the notebook you will find a few tips and frequently made mistakes. If you feel enough might to shoot yourself in the foot without external assistance, we encourage you to do so, but if you encounter any unsurpassable issues, please do look there before mailing us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "s = tf.InteractiveSession()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "\n",
    "#[down]loading the original MNIST dataset.\n",
    "#Please note that you should only train your NN on _train sample,\n",
    "# _val can be used to evaluate out-of-sample error, compare models or perform early-stopping\n",
    "# _test should be hidden under a rock untill final evaluation... But we both know it is near impossible to catch you evaluating on it.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.one_hot(y_train, num_classes).eval()\n",
    "y_val = tf.one_hot(y_val, num_classes).eval()\n",
    "y_test = tf.one_hot(y_test, num_classes).eval()\n",
    "\n",
    "image_shape = (28, 28)\n",
    "X_train = np.reshape(X_train, (len(X_train),) + image_shape + (1,))\n",
    "X_val = np.reshape(X_val, (len(X_val),) + image_shape + (1,))\n",
    "X_test = np.reshape(X_test, (len(X_test),) + image_shape + (1,))\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = tf.placeholder('float32', shape=(None,) + image_shape + (1,))\n",
    "input_y = tf.placeholder('float32', shape=(None, num_classes))\n",
    "\n",
    "# if you want a particular initialization, check docs and find it there\n",
    "# i.e., for Glorot Uniform you may import the following\n",
    "# from tf.contrib.keras.initializers import glorot_uniform\n",
    "\n",
    "# below is just an EXAMPLE! You can change everything\n",
    "\n",
    "with tf.variable_scope('MyNN'):\n",
    "    nn = tf.layers.conv2d(\n",
    "        input_X, 32, 3, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "        bias_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    nn = tf.layers.max_pooling2d(nn, 2, 2)\n",
    "    nn = tf.layers.batch_normalization(nn)\n",
    "    nn = tf.layers.conv2d(\n",
    "        nn, 64, 3, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "        bias_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    nn = tf.layers.max_pooling2d(nn, 2, 2)\n",
    "    nn = tf.layers.batch_normalization(nn)\n",
    "    \n",
    "    nn = tf.layers.flatten(nn)\n",
    "\n",
    "    nn = tf.layers.dense(\n",
    "        nn, 512, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "        bias_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    nn = tf.layers.dense(\n",
    "        nn, 512, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "        bias_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "\n",
    "    predicted_y = tf.layers.dense(\n",
    "        nn, num_classes,\n",
    "        kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "        bias_initializer=tf.glorot_uniform_initializer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    variable\n",
    "    for variable in tf.global_variables()\n",
    "    if variable.name.startswith('MyNN')\n",
    "]\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predicted_y, labels=input_y))\n",
    "learning_rate = tf.Variable(0.001)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, var_list=(weights,))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "    tf.argmax(predicted_y, 1),\n",
    "    tf.argmax(input_y, 1)),\n",
    "    \"float\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAYLEHSBI3AGCBuBoeaOjjCPZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce31afc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "sess_large = tf.Session()\n",
    "sess_large.run(tf.global_variables_initializer())\n",
    "\n",
    "def mnist_batch_generator(in_x, in_y, batch_size):\n",
    "    random_indeces = np.random.permutation(len(in_x))[:batch_size]\n",
    "    return in_x[random_indeces], in_y[random_indeces]\n",
    "\n",
    "# We'll train in minibatches and report accuracy:\n",
    "batch_size = 32\n",
    "n_epochs = 500\n",
    "\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for epoch_i in range(n_epochs):\n",
    "    for batch_i in range(int(n_epochs / batch_size)):        \n",
    "        batch_xs, batch_ys = mnist_batch_generator(X_train, y_train, batch_size)\n",
    "        sess_large.run(\n",
    "            optimizer, feed_dict={\n",
    "                input_X: batch_xs, input_y: batch_ys\n",
    "            }\n",
    "        )\n",
    "\n",
    "    val_scores.append(sess_large.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            input_X: X_val,\n",
    "            input_y: y_val\n",
    "        }\n",
    "    ))\n",
    "    X_train_batch, y_train_batch = mnist_batch_generator(X_train, y_train, len(X_val))\n",
    "    train_scores.append(sess_large.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            input_X: X_train_batch,\n",
    "            input_y: y_train_batch\n",
    "        }\n",
    "    ))\n",
    "    clear_output(True)\n",
    "    plt.plot(val_scores, color='blue', label='val score: {}'.format(val_scores[-1]))\n",
    "    plt.plot(train_scores, color='red', label='train score: {}'.format(train_scores[-1]))\n",
    "    plt.title(\"Scores after {}//{} epochs\".format(epoch_i, n_epochs))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if train_scores[-1] > 0.98:\n",
    "        learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy at test 0.9905\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"accuracy at test\",\n",
    "    sess_large.run(\n",
    "        accuracy,\n",
    "        feed_dict={\n",
    "            input_X: X_test,\n",
    "            input_y: y_test\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Сначала сделал 2 полносвязных слоя. Они дали качество в районе 0.95. Потом добавил 2 conv'а и сделал, чтобы рисовались графики ошибки при обучении. Качество выросло до 0.97 с копйками. Затем добавил усреднение по батчу, уменьшение темпа обучания, когда качество уже достаточно большое и исправил немного ошибок (например, по ошибка второй conv состоял, как и первый, из 32 фильтров, а хотелось 64). Качество на тесте 0.99, правда это скорее удачное стечение обстоятельств, в реальности оно варируется от 0.985 до 0.99. Задание полезное, т.к. чисто на tf я раньше не писал."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# SPOILERS!\n",
    "\n",
    "Recommended pipeline\n",
    "\n",
    "* Adapt logistic regression from previous assignment to classify some number against others (e.g. zero vs nonzero)\n",
    "* Generalize it to multiclass logistic regression.\n",
    "  - Either try to remember lecture 0 or google it.\n",
    "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "  - softmax (exp over sum of exps) can implemented manually or as T.nnet.softmax (stable)\n",
    "  - probably better to use STOCHASTIC gradient descent (minibatch)\n",
    "    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n",
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
    "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (sigmoid) instead of softmax\n",
    "  - You need to train both layers, not just output layer :)\n",
    "  - Do not initialize layers with zeros (due to symmetry effects). A gaussian noize with small sigma will do.\n",
    "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n",
    "  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n",
    "  - __make sure this neural network works better than logistic regression__\n",
    "  \n",
    "* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
